{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wON2nVET2set",
        "tags": []
      },
      "source": [
        "\n",
        "# Multi-armed bandits\n",
        "## Demo and Preliminaries\n",
        "\n",
        "We will begin by getting familiar with the basic problem setup before we dig in to the actual assignement problems. Let's start by loading all the required libraries for this notebook. For generating plots you must have `ipympl` installed and `jupyter-matplotlib` extension installed and enabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H89KRE9N2seu",
        "outputId": "56c6f45e-5b2e-4a12-dbd0-f25047f1e83d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Requirement already satisfied: pip in c:\\users\\shali\\anaconda3\\lib\\site-packages (24.0)\n",
            "Requirement already satisfied: gymnasium in c:\\users\\shali\\anaconda3\\lib\\site-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from gymnasium) (1.24.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from gymnasium) (4.7.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: ipympl in c:\\users\\shali\\anaconda3\\lib\\site-packages (0.9.3)\n",
            "Requirement already satisfied: ipython<9 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipympl) (8.15.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipympl) (1.24.3)\n",
            "Requirement already satisfied: ipython-genutils in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipympl) (0.2.0)\n",
            "Requirement already satisfied: pillow in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipympl) (10.0.1)\n",
            "Requirement already satisfied: traitlets<6 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipympl) (5.7.1)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipympl) (8.0.4)\n",
            "Requirement already satisfied: matplotlib<4,>=3.4.0 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipympl) (3.7.2)\n",
            "Requirement already satisfied: backcall in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipython<9->ipympl) (0.2.0)\n",
            "Requirement already satisfied: decorator in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipython<9->ipympl) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipython<9->ipympl) (0.18.1)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipython<9->ipympl) (0.1.6)\n",
            "Requirement already satisfied: pickleshare in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipython<9->ipympl) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipython<9->ipympl) (3.0.36)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipython<9->ipympl) (2.15.1)\n",
            "Requirement already satisfied: stack-data in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipython<9->ipympl) (0.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipython<9->ipympl) (0.4.6)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipywidgets<9,>=7.6.0->ipympl) (6.25.0)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipywidgets<9,>=7.6.0->ipympl) (4.0.5)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib<4,>=3.4.0->ipympl) (1.0.5)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib<4,>=3.4.0->ipympl) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib<4,>=3.4.0->ipympl) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib<4,>=3.4.0->ipympl) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib<4,>=3.4.0->ipympl) (23.1)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib<4,>=3.4.0->ipympl) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib<4,>=3.4.0->ipympl) (2.8.2)\n",
            "Requirement already satisfied: comm>=0.1.1 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (0.1.2)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (1.6.7)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (7.4.9)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (5.3.0)\n",
            "Requirement already satisfied: nest-asyncio in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (1.5.6)\n",
            "Requirement already satisfied: psutil in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (5.9.0)\n",
            "Requirement already satisfied: pyzmq>=20 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (23.2.0)\n",
            "Requirement already satisfied: tornado>=6.1 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.3.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\shali\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython<9->ipympl) (0.2.5)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib<4,>=3.4.0->ipympl) (1.16.0)\n",
            "Requirement already satisfied: executing in c:\\users\\shali\\anaconda3\\lib\\site-packages (from stack-data->ipython<9->ipympl) (0.8.3)\n",
            "Requirement already satisfied: asttokens in c:\\users\\shali\\anaconda3\\lib\\site-packages (from stack-data->ipython<9->ipympl) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in c:\\users\\shali\\anaconda3\\lib\\site-packages (from stack-data->ipython<9->ipympl) (0.2.2)\n",
            "Requirement already satisfied: entrypoints in c:\\users\\shali\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (0.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (3.10.0)\n",
            "Requirement already satisfied: pywin32>=300 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (305.1)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\shali\\anaconda3\\lib\\site-packages (3.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib) (10.0.1)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\shali\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\shali\\anaconda3\\lib\\site-packages (4.65.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\shali\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "!python.exe -m pip install --upgrade pip\n",
        "!pip install \"gymnasium\"\n",
        "!pip install ipympl\n",
        "!pip install matplotlib\n",
        "!pip install tqdm\n",
        "%matplotlib widget\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O01qmZN2sev"
      },
      "source": [
        "## Scenario\n",
        "\n",
        "The Multi-Armed Bandit describes a situation in which an agent has only one state, and multiple actions to interact with the environment. Each action gives a random reward, centered on an unkown value. Our agent wants to maximize the reward received, which means it wants to find the action that yields a higher reward. Based on this, let's build the scenario.\n",
        "\n",
        "We're going to implement the multi-armed bandit environment using OpenAI's gym interface. Also, our implementation will be vectorized for the sake of optimization. This allows us to run multiple agents on multiple environments at the same time. The code below is based on [this implementation](https://github.com/diegoalejogm/openai-k-armed-bandits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz6nX5I_2sev"
      },
      "outputs": [],
      "source": [
        "from gymnasium import spaces\n",
        "from gymnasium.utils import seeding\n",
        "import gymnasium as gym\n",
        "\n",
        "class ArmedBanditsEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    The famous k-Armed Bandit Environment, implemented for the gym interface.\n",
        "    Initialization requires an array of length equals to k, where each item is\n",
        "    a function which samples from a specified distribution.\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, mean, stddev):\n",
        "        assert len(mean.shape) == 2\n",
        "        assert len(stddev.shape) == 2\n",
        "\n",
        "        super(ArmedBanditsEnv, self).__init__()\n",
        "        # Define action and observation space\n",
        "        self.num_bandits = mean.shape[1]\n",
        "        self.num_experiments = mean.shape[0]\n",
        "        self.action_space = spaces.Discrete(self.num_bandits)\n",
        "\n",
        "        # Theres one state only in the k-armed bandits problem\n",
        "        self.observation_space = spaces.Discrete(1)\n",
        "        self.mean = mean\n",
        "        self.stddev = stddev\n",
        "\n",
        "    def step(self, action):\n",
        "        # Sample from the specified bandit using it's reward distribution\n",
        "        assert (action < self.num_bandits).all()\n",
        "\n",
        "        sampled_means = self.mean[np.arange(self.num_experiments),action]\n",
        "        sampled_stddevs = self.stddev[np.arange(self.num_experiments),action]\n",
        "\n",
        "        reward = np.random.normal(loc=sampled_means, scale=sampled_stddevs, size=(1,self.num_experiments))\n",
        "\n",
        "        # Return a constant state of 0. Our environment has no terminal state\n",
        "        observation, done, info = 0, False, dict()\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        return 0\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        pass\n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np.random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class ArmedBanditsGaussian(ArmedBanditsEnv):\n",
        "    def __init__(self, num_experiments=1, num_bandits=3):\n",
        "        self.means = np.random.normal(size=(num_experiments, num_bandits))\n",
        "\n",
        "        ArmedBanditsEnv.__init__(self, self.means, np.ones((num_experiments, num_bandits)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWmKZ0uX2sew"
      },
      "source": [
        "Here we're creating two classes, `ArmedBanditsEnv` is the main class, while `ArmedBanditsGaussian` is an auxiliary class that allows us to easily create an environment with random mean rewards for each action. Our environment receives numpy arrays for the means and standard deviations for each action. The dimensions of this arrays are described as `num_experiments`x`num_bandits`. Taking a step requires a numpy vector of size `num_experiments`, where each value specifies which action to take for each experiment. The step functions returns, among other information, a vector of reward obtained for each experiment. Let's see this in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc9rBv-T2sex",
        "outputId": "73f66c80-6c1b-4fc5-e7a8-b7f780f934c6"
      },
      "outputs": [],
      "source": [
        "means = np.array([[5, 1, 0, -10]]) # The mean for a four-armed bandit. Single experiment\n",
        "stdev = np.array([[1, 0.1, 5, 1]]) # The standard deviation for a four-armed bandit.\n",
        "\n",
        "env = ArmedBanditsEnv(means, stdev) # Create the environment\n",
        "\n",
        "for i in range(4):\n",
        "    action = np.array([[i]])\n",
        "    _, reward, _, _ = env.step(action)\n",
        "    print(\"Bandit:\", i, \" gave a reward of:\", reward[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSsziz9u2sey"
      },
      "source": [
        "## Evaluating our actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8g7dhvZ2sez"
      },
      "source": [
        "In order to learn something from our interaction with the environment, we need to know how exactly we determine the value of our actions, as well as how to keep that value up to date. A simple strategy our agents can take is to calculate the expected return for each action. This can be done through experience by taking the average of previous rewards given by a determined action.\n",
        "\n",
        "For example, we can use the next list of rewards to calculate the expected return of the chosen action:\n",
        "\n",
        "$\\bar{\\mu}_n = \\dfrac{R_1+R_2+\\dots+R_n}{n}$\n",
        "\n",
        "Having to store all the previously seen rewards to calculate the value of an action is cumbersome, inefficient and unnecessary. We can derive another form of average, called *Incremental average update rule*, which only requires us to know the previous average $\\bar{\\mu}_{n-1}$ and the number $n$:\n",
        "\n",
        "$\\bar{\\mu}_n = \\bar{\\mu}_{n-1} + \\dfrac{1}{n}(R_n - \\bar{\\mu}_{n-1})$\n",
        "\n",
        "Here's a small demonstration of this function at work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hvBWbzZ2sez",
        "outputId": "645bd8fe-9160-4774-fdca-f02680baeb79"
      },
      "outputs": [],
      "source": [
        "def inc_avg(prev_avg, new_val, n):\n",
        "    return prev_avg + 1/n*(new_val - prev_avg)\n",
        "\n",
        "# Obtain the previous average\n",
        "vals = np.array([4.5, 5.04, 5.32, 4.8, 5.11])\n",
        "prev_avg = vals.mean()\n",
        "\n",
        "# Calculate a new average using the incremental average update function\n",
        "new_val = 5.18\n",
        "new_avg = inc_avg(prev_avg, new_val, 6)\n",
        "\n",
        "# Calculate the same average using all previous values for comparison\n",
        "avg = np.append(vals, new_val).mean()\n",
        "\n",
        "print(\"Average obtained from incremental update rule: \", new_avg)\n",
        "print(\"Average obtained from basic average function:  \", avg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di0QdX_n2sez"
      },
      "source": [
        "This method will be used on the next strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEuDOZ852se0"
      },
      "source": [
        "## The Greedy Agent\n",
        "\n",
        "This strategy is focused on always choosing the best known action at the time. Every time the agent takes an action, it looks at the estimated values for each action, and chooses the one that has a greater score. If more than two values look best, then the agent selects arbitrarily among those best-valued actions. This is called breaking ties arbitrarily\n",
        "\n",
        "Taking the action with the greatest value is equivalent to using the **argmax** function. Although, we need to implement some changes so that the funciton breaks the ties the way we intend it to do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFNJoUd92se0"
      },
      "outputs": [],
      "source": [
        "def argmax(q_values):\n",
        "    \"\"\"\n",
        "    Takes in a matrix of n*k q_values and returns the index\n",
        "    of the item with the highest value for each row.\n",
        "    Breaks ties randomly.\n",
        "    returns: vector of size n, where each item is the index of\n",
        "    the highest value in q_values for each row.\n",
        "    \"\"\"\n",
        "    # Generate a mask of the max values for each row\n",
        "    mask = q_values == q_values.max(axis=1)[:, None]\n",
        "    # Generate noise to be added to the ties\n",
        "    r_noise = 1e-6*np.random.random(q_values.shape)\n",
        "    # Get the argmax of the noisy masked values\n",
        "    return np.argmax(r_noise*mask,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfXWUrgO2se0"
      },
      "source": [
        "Here we're generating a mask of all the values in the input that are equal to the maximum value. Then ,we generate some noise and multiply it with the mask. Taking the argmax of this new list will be equivalent to the desired argmax with ties broken arbitrarily.\n",
        "\n",
        "Now that we have specified the argmax function, we can declare our class for the Greedy Agent, which is capable of acting upon the environment, and updating its estimates after receiving a reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUjX3Vz02se1"
      },
      "outputs": [],
      "source": [
        "class GreedyAgent:\n",
        "    def __init__(self, reward_estimates):\n",
        "        \"\"\"\n",
        "        Our agent takes as input the initial reward estimates.\n",
        "        This estimates will be updated incrementally after each\n",
        "        interaction with the environment.\n",
        "        \"\"\"\n",
        "        assert len(reward_estimates.shape) == 2\n",
        "\n",
        "        self.num_bandits = reward_estimates.shape[1]\n",
        "        self.num_experiments = reward_estimates.shape[0]\n",
        "        self.reward_estimates = reward_estimates.astype(np.float64)\n",
        "        self.action_count = np.zeros(reward_estimates.shape)\n",
        "\n",
        "    def get_action(self):\n",
        "        # Our agent is greedy, so there's no need for exploration.\n",
        "        # Our argmax will do just fine for this situation\n",
        "        action = argmax(self.reward_estimates)\n",
        "\n",
        "        # Add a 1 to each action selected in the action count\n",
        "        self.action_count[np.arange(self.num_experiments), action] += 1\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update_estimates(self, reward, action):\n",
        "        # rew is a matrix with the obtained rewards from our previuos\n",
        "        # action. Use this to update our estimates incrementally\n",
        "        n = self.action_count[np.arange(self.num_experiments), action]\n",
        "        prev_reward_estimates = self.reward_estimates[np.arange(self.num_experiments), action]\n",
        "\n",
        "        # Update the reward estimates incementally\n",
        "        self.reward_estimates[np.arange(self.num_experiments), action] = inc_avg(prev_reward_estimates,reward,n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkLDTxGg2se1"
      },
      "source": [
        "The greedy agent contains a matrix for the estimates, as well as for the number of times each action has been taken. This is necessary for using the incremental average update rule, used inside the `update_estimates` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQmJ4qtp2se1"
      },
      "source": [
        "### Testing the agent\n",
        "\n",
        "Let's see how the Greedy Agent behaves on the environment. For this, we're going to generate some animations where we're able to see how the agent estimates the values for each action, as well as the real values provided by the environment. Remember that the agent doesn't have access to that information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "Z_jM_1_W2se1",
        "outputId": "f7b7250a-d084-4e87-a924-dc87518c5ec1"
      },
      "outputs": [],
      "source": [
        "# Initialize the environment of our multi-armed bandit problem\n",
        "num_experiments = 2\n",
        "num_bandits = 8\n",
        "num_steps = 100\n",
        "means = np.random.normal(size=(num_experiments, num_bandits))\n",
        "stdev = np.ones((num_experiments, num_bandits))\n",
        "\n",
        "env = ArmedBanditsEnv(means, stdev)\n",
        "\n",
        "# Initialize the agent\n",
        "agent = GreedyAgent(np.zeros((num_experiments,num_bandits)))\n",
        "\n",
        "# Code for plotting the interaction\n",
        "fig, axs = plt.subplots(1, num_experiments, figsize=(10, 4))\n",
        "x_pos = np.arange(num_bandits)\n",
        "\n",
        "def init():\n",
        "    axa = [];\n",
        "    for i in range(num_experiments):\n",
        "        init_ax(i)\n",
        "        # axa.append(init_ax(i))\n",
        "    # return axa\n",
        "\n",
        "def init_ax(i):\n",
        "    ax = axs[i]\n",
        "    ax.clear()\n",
        "    ax.set_ylim(-4, 4)\n",
        "    ax.set_xlim(-0.5, num_bandits-.5)\n",
        "    ax.set_xlabel('Actions', fontsize=14)\n",
        "    ax.set_ylabel('Value', fontsize=14)\n",
        "    ax.set_title(label='Estimated Values vs. Real values', fontsize=15)\n",
        "    ax.plot(x_pos, env.mean[i], marker='D', linestyle='', alpha=0.8, color='r', label='Real Values')\n",
        "    ax.axhline(0, color='black', lw=1)\n",
        "    # return ax\n",
        "\n",
        "\n",
        "# Implement a step, which involves the agent acting upon the\n",
        "# environment and learning from the received reward.\n",
        "def step(g):\n",
        "    action = agent.get_action()\n",
        "    _, reward, _, _ = env.step(action)\n",
        "    agent.update_estimates(reward, action)\n",
        "    axa = []\n",
        "    for i in range(num_experiments):\n",
        "        ax = axs[i]\n",
        "        # Plot the estimated values from the agent compared to the real values\n",
        "        estimates = agent.reward_estimates[i]\n",
        "        init_ax(i)\n",
        "        values = ax.bar(x_pos, estimates, align='center', color='blue', alpha=0.4, label='Estimated Values')\n",
        "        ax.legend()\n",
        "        # axa.append(ax)\n",
        "    # return axa\n",
        "\n",
        "anim = FuncAnimation(fig, func=step, frames=num_steps, init_func=init, interval=10, repeat=False, blit=True)\n",
        "plt.show()\n",
        "\n",
        "# Uncomment the next line if you wish to store the animations as a gif\n",
        "# anim.save('./greedy-agent.gif', writer='imagemagick', fps=60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liI8-lBU2se2"
      },
      "source": [
        "you may try the previous block of code multiple times to see multiple animations. In general, you may realize that most of the actions are not explored by the agent, and that it will mostly stick to the one action that gave it some positive reward. The agent is good at evading negative values, but it will only land on the optimal action by pure chance.\n",
        "\n",
        "We can plot the average behavior by doing more experiments. The next plot displays the percentage of times the agent chose the optimal action over an average of 10000 experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "mBAlPJxg2se2",
        "outputId": "49f8e83b-2c7f-4681-8fdb-8c353c7585d6"
      },
      "outputs": [],
      "source": [
        "num_experiments = 10000\n",
        "num_steps = 1000\n",
        "num_actions = 10\n",
        "\n",
        "# Initialize the environment\n",
        "env = ArmedBanditsGaussian(num_experiments, num_actions)\n",
        "# Initialize the agent\n",
        "agent = GreedyAgent(np.zeros((num_experiments, num_actions)))\n",
        "\n",
        "# Store the scores and averages for later plotting\n",
        "averages = np.zeros((num_steps))\n",
        "optimality = np.zeros((num_steps))\n",
        "scores = np.zeros((num_experiments, num_steps+1))\n",
        "\n",
        "#Store the optimal actions for later use\n",
        "optimal = np.argmax(env.mean, axis=1)\n",
        "\n",
        "for i in tqdm(range(num_steps)):\n",
        "    # Select an action to execute on the environment\n",
        "    action = agent.get_action()\n",
        "    _, reward, _, _ = env.step(action)\n",
        "\n",
        "    # Update the agent estimates with the previously observed rewards\n",
        "    agent.update_estimates(reward, action)\n",
        "\n",
        "    # Store the average cumulative score and optimality of the current step\n",
        "    scores[:,i+1] = scores[:,i] + reward\n",
        "    avg_score = np.mean(scores[:,i+1]/(i+1))\n",
        "    averages[i] = avg_score\n",
        "\n",
        "    # Get optimal actions from the environment\n",
        "    current_optimality = np.mean(action == optimal)\n",
        "    optimality[i] = current_optimality\n",
        "\n",
        "plt.figure(figsize=(12,6), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.plot([1.0 for _ in range(num_steps)], linestyle='--')\n",
        "plt.plot([0.0 for _ in range(num_steps)], linestyle='--')\n",
        "plt.plot(optimality)\n",
        "plt.legend([\"Best Possible\", \"Worst Possible\", \"Greedy\"])\n",
        "plt.title(\"Average Optimality of Greedy Agent\", fontsize=14)\n",
        "plt.ylabel(\"% of Optimal Action Taken\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.show()\n",
        "greedy_scores = averages\n",
        "greedy_optimality = optimality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWa-6MVh2se3"
      },
      "source": [
        "The Greedy Agent chooses the best action on average ~40% of the times. Additionally, extra experience won't improve it's score, as it usually lands on a sub-optimal action on the first steps, and stays there for ever. The Greedy Agent is not a good strategy for finding the optimal action on this scenario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKr4rdEa2se3"
      },
      "source": [
        "## The Epsilon-Greedy Agent\n",
        "\n",
        "Another strategy is the Epsilon Greedy Agent, which adds to the previous strategy. An Epsilon-Greedy Agent allows for some exploratory actions, by every once in a while choosing any action randomly, instead of always acting greedily. The probability of taking an exploratory action is defined by the parameter `epsilon`. An epsilon of `0` is a Greedy Agent. An epsilon of `0.2` means our agent takes a random action 20% of the time. An epsilon of `1` is an agent that behaves randomly. Let's build this new agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Poi42r3d2se3"
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedyAgent(GreedyAgent):\n",
        "    def __init__(self, reward_estimates, epsilon):\n",
        "        GreedyAgent.__init__(self, reward_estimates)\n",
        "        # Store the epsilon value\n",
        "        assert epsilon >= 0 and epsilon <= 1\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def get_action(self):\n",
        "        # We need to redefine this function so that it takes an exploratory action with epsilon probability\n",
        "\n",
        "        # One hot encoding: 0 if exploratory, 1 otherwise\n",
        "        action_type = (np.random.random_sample(self.num_experiments) > self.epsilon).astype(int)\n",
        "        # Generate both types of actions for every experiment\n",
        "        exploratory_action = np.random.randint(self.num_bandits, size=self.num_experiments)\n",
        "        greedy_action = argmax(self.reward_estimates)\n",
        "        # Use the one hot encoding to mask the actions for each experiment\n",
        "        action = greedy_action * action_type + exploratory_action * (1 - action_type)\n",
        "\n",
        "        self.action_count[np.arange(self.num_experiments), action] += 1\n",
        "\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1KQFxgD2se3"
      },
      "source": [
        "As may be seen, we're inheriting the `GreedyAgent` implementation, and only changing the `get_action` function. For taking an action, we're using a masking that defines which experiments will choose an action randomly and which will act greedily. Then, we apply that masking between random and greedy actions to obtain the action vector for all of our experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_hRV_Xk2se6"
      },
      "source": [
        "### Testing the Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPE2O8AU2se6"
      },
      "source": [
        "Here, we're going to use the same animation as before to observe how this new strategy behaves. Our agents will have a value of epsilon of `0.1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "80e4bb527a0c4774a097a1534b213383",
            "f7bf92826b1e4d0ea70266bd394268c4"
          ]
        },
        "id": "SWWsxxsa2se7",
        "outputId": "a2bf3f66-e4dd-482d-a95e-083ed5647110"
      },
      "outputs": [],
      "source": [
        "# Initialize the environment of our multi-armed bandit problem\n",
        "num_experiments = 2\n",
        "num_bandits = 8\n",
        "num_steps = 200\n",
        "means = np.random.normal(size=(num_experiments, num_bandits))\n",
        "stdev = np.ones((num_experiments, num_bandits))\n",
        "\n",
        "env = ArmedBanditsEnv(means, stdev)\n",
        "\n",
        "# Initialize the agent\n",
        "agent = EpsilonGreedyAgent(np.zeros((num_experiments,num_bandits)), 0.1)\n",
        "\n",
        "# Code for plotting the interaction\n",
        "fig, axs = plt.subplots(1, num_experiments, figsize=(10, 4))\n",
        "x_pos = np.arange(num_bandits)\n",
        "\n",
        "def init():\n",
        "    for i in range(num_experiments):\n",
        "        init_ax(i)\n",
        "\n",
        "\n",
        "def init_ax(i):\n",
        "    ax = axs[i]\n",
        "    ax.clear()\n",
        "    ax.set_ylim(-4, 4)\n",
        "    ax.set_xlim(-0.5, num_bandits-.5)\n",
        "    ax.set_xlabel('Actions', fontsize=14)\n",
        "    ax.set_ylabel('Value', fontsize=14)\n",
        "    ax.set_title(label='Estimated Values vs. Real values', fontsize=15)\n",
        "    ax.plot(x_pos, env.mean[i], marker='D', linestyle='', alpha=0.8, color='r', label='Real Values')\n",
        "    ax.axhline(0, color='black', lw=1)\n",
        "\n",
        "# Implement a step, which involves the agent acting upon the\n",
        "# environment and learning from the received reward.\n",
        "def step(g):\n",
        "    action = agent.get_action()\n",
        "    _, reward, _, _ = env.step(action)\n",
        "    agent.update_estimates(reward, action)\n",
        "    for i in range(num_experiments):\n",
        "        ax = axs[i]\n",
        "        # Plot the estimated values from the agent compared to the real values\n",
        "        estimates = agent.reward_estimates[i]\n",
        "        init_ax(i)\n",
        "        values = ax.bar(x_pos, estimates, align='center', color='blue', alpha=0.4, label='Estimated Values')\n",
        "        ax.legend()\n",
        "\n",
        "anim = FuncAnimation(fig, func=step, frames=np.arange(num_steps), init_func=init, interval=10, repeat=False, blit=True)\n",
        "plt.show()\n",
        "\n",
        "# Uncomment the next line if you wish to store the animations as a gif\n",
        "# anim.save('./epsilon-greedy-agent.gif', writer='imagemagick', fps=60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQBszQNX2se7"
      },
      "source": [
        "Again, you may run the previous block multiple times to see different runs. As can be observed, the epsilon-greedy agent not only finds the optimal solution most of the times, but it is also capable of finding close estimated values for all the actions! How does our new strategy compares to the greedy agent on average?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUk7GyL22se7"
      },
      "outputs": [],
      "source": [
        "def run_experiment(num_experiments=1000, num_steps=1000, num_actions=10,epsilon=0.1):\n",
        "    # Initialize the environment\n",
        "    env = ArmedBanditsGaussian(num_experiments, num_actions)\n",
        "    # Initialize the agent\n",
        "    agent = EpsilonGreedyAgent(np.zeros((num_experiments, num_actions)), epsilon)\n",
        "\n",
        "    # Store the scores and averages for later plotting\n",
        "    averages = np.zeros((num_steps))\n",
        "    optimality = np.zeros((num_steps))\n",
        "    scores = np.zeros((num_experiments, num_steps+1))\n",
        "\n",
        "    #Store the optimal actions for later use\n",
        "    optimal = np.argmax(env.mean, axis=1)\n",
        "\n",
        "    for i in tqdm(range(num_steps)):\n",
        "        # Select an action to execute on the environment\n",
        "        action = agent.get_action()\n",
        "        _, reward, _, _ = env.step(action)\n",
        "\n",
        "        # Update the agent estimates with the previously observed rewards\n",
        "        agent.update_estimates(reward, action)\n",
        "\n",
        "        # Store the average cumulative score and optimality of the current step\n",
        "        scores[:,i+1] = scores[:,i] + reward\n",
        "        avg_score = np.mean(scores[:,i+1]/(i+1))\n",
        "        averages[i] = avg_score\n",
        "\n",
        "        # Get optimal actions from the environment\n",
        "        current_optimality = np.mean(action == optimal)\n",
        "        optimality[i] = current_optimality\n",
        "    return optimality, averages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ee23248ec89842c38e0a0ceb72fc232b",
            "b929f3fc9e414c4db31e2c9a00e617b1"
          ]
        },
        "id": "RLNOrcM82se7",
        "outputId": "edab55e8-00ab-4a91-ad48-da7a5c159836"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'run_experiment' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m num_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      4\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m----> 6\u001b[0m balanced_eps_optimality, balanced_eps_scores \u001b[38;5;241m=\u001b[39m run_experiment(num_experiments, num_steps, num_actions, epsilon)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m6\u001b[39m), dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps)], linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'run_experiment' is not defined"
          ]
        }
      ],
      "source": [
        "num_experiments = 10000\n",
        "num_steps = 1000\n",
        "num_actions = 10\n",
        "epsilon = 0.1\n",
        "\n",
        "balanced_eps_optimality, balanced_eps_scores = run_experiment(num_experiments, num_steps, num_actions, epsilon)\n",
        "\n",
        "plt.figure(figsize=(12,6), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.plot([1.0 for _ in range(num_steps)], linestyle='--')\n",
        "plt.plot([0.0 for _ in range(num_steps)], linestyle='--')\n",
        "plt.plot(balanced_eps_optimality)\n",
        "plt.plot(greedy_optimality)\n",
        "plt.legend([\"Best Possible\", \"Worst Possible\", \"0.1 $\\epsilon$ Greedy\", \"Greedy\"])\n",
        "plt.title(\"Average Optimality of Greedy Agent vs Epsilon-Greedy Agent\")\n",
        "plt.ylabel(\"% of Optimal Action Taken\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6yXM3Rw2se8"
      },
      "source": [
        "After 1000 steps, the Epsilon-Greedy Agent is capable of reaching an 80% average optimality. This means that on average, they choose the best possible action 80% of the times after that amount of iterations. We can also see how our agent is capable of improving from experience, contrary to the Greedy Agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOGGfJRm2se8"
      },
      "source": [
        "## Values for epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zx_9bwy2se8"
      },
      "source": [
        "On the previous example, we used a value for `epsilon` of `0.1`. This means that the agent chose an exploratory action 10% of the times. What would have happened if we chose another action? Like with almost all hyper-parameters in Machine Learning, there's a range of values which behave better than others. Let's make an actual experiment using different values for `epsilon`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c99d5001fc2a433e8f37c90ddc673b5d",
            "c7cf0cf405e24d7880b8277334aff30b"
          ]
        },
        "id": "G-OedWQ72se8",
        "outputId": "65281e44-d1c6-47f3-cf38-1d6af30e862a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m      3\u001b[0m num_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m----> 4\u001b[0m epsilons \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.4\u001b[39m])\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m6\u001b[39m), dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# plt.plot([1.6 for _ in range(num_steps)], linestyle='--') # why 1.6?\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "num_experiments = 1000\n",
        "num_steps = 1000\n",
        "num_actions = 10\n",
        "epsilons = np.array([0.0, 0.01, 0.1, 0.4])\n",
        "\n",
        "plt.figure(figsize=(12,6), dpi=80, facecolor='w', edgecolor='k')\n",
        "# plt.plot([1.6 for _ in range(num_steps)], linestyle='--') # why 1.6?\n",
        "\n",
        "\n",
        "for epsilon in epsilons:\n",
        "    _, reward = run_experiment(num_experiments, num_steps, num_actions, epsilon)\n",
        "\n",
        "    plt.plot(reward)\n",
        "\n",
        "# plt.legend([\"Best Possible\"] + epsilons.tolist())\n",
        "plt.legend(epsilons.tolist())\n",
        "plt.title(\"Average reward for multiple values of epsilon\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2NwECSH2se8"
      },
      "source": [
        "Here, we're plotting the average amount of reward received by the agent. This is because plotting the optimality on this many tests gets too crowded and noisy. As can be seen, the agent with an epsilon of `0.1` was the one that received the most amount of reward. Values of epsilon that are too small take very few exploratory actions, and therefore take a long time to explore and find the most optimal value. On the other hand, values of `epsilon` too high will obstruct the capacity of the agent of acting optimally. Too much exploration gets in the way of exploitation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32BlCjd3HvId"
      },
      "source": [
        "***\n",
        "## Part B questions\n",
        "\n",
        "Now that we have familiarized ourselves with the basic setup let us test a few more stratergies as part of the assignment problem.\n",
        "\n",
        "Consider a two-armed Bernoulli bandit scenario with true means given by $\\mu_1 = \\frac{1}{2}, \\mu_2= \\frac{1}{2}+\\Delta$, for some $\\Delta < \\frac{1}{2}$. Let the time horizon be $T=10000$. `[20 Marks]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### B1\n",
        "Take $\\Delta=\\frac{1}{4}$ and run the Monte Carlo simulations to estimate the expected regret of the ETC algorithm which explores each arm $m = T^{2/3} (\\log T)^{1/3}$ times before committing. Specifically, you run the ETC algorithm to compute the sample regret \n",
        "$$ \\mu_2 * T - \\sum_{t=1}^T R_t, $$ \n",
        "where $R_t$ is the reward obtained in time step $t$.\n",
        "\n",
        "Repeat this experiment 500 times and estimate the expected regret by taking the average of the sample regrets you obtained in all those 500 experiments. `[5 Marks]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated Expected Regret : 5.582\n"
          ]
        }
      ],
      "source": [
        "# write your code for the above part here\n",
        "# Import necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "from gymnasium.utils import seeding\n",
        "import gymnasium as gym\n",
        "\n",
        "class ArmedBanditsEnv(gym.Env):\n",
        "\n",
        "    def __init__(self, delta):\n",
        "        super(ArmedBanditsEnv, self).__init__()\n",
        "        #define action and observation space\n",
        "        self.delta = delta\n",
        "        self.action_space = spaces.Discrete(2) #two arms\n",
        "        self.observation_space = spaces.Discrete(1) #discrete choices between two options\n",
        "        self.true_means = [0.5, 0.5 + self.delta]  #armed bandit parameters\n",
        "        self.step_count = 0  #tracking the current time step\n",
        "        self.seed_value = None\n",
        "\n",
        "    def step(self, action):\n",
        "        #stimulate bandit step by taking action(choosing arm)\n",
        "        reward = np.random.binomial(1, self.true_means[action])\n",
        "        self.step_count += 1\n",
        "        done = self.step_count >= T #if time horizon is reached\n",
        "        return np.array([0.0]), reward, done, {}\n",
        "    \n",
        "    def reset(self):  \n",
        "        self.step_count = 0  #reseting the environment to the initial state\n",
        "        return np.array([0.0])\n",
        "    \n",
        "    def render(self):\n",
        "        print(f\"Step: {self.step_count}, True Mreans: {self.true_means}\")\n",
        "\n",
        "    def seed(self, seed = None):\n",
        "        self.seed_value = seed\n",
        "        np.random.seed(seed)\n",
        "    \n",
        "    def close(self):\n",
        "        pass #cleanup and closing logic\n",
        "\n",
        "#ETC Algorithm\n",
        "def etc_algorithm(env, T, exploration_factor):\n",
        "        #simulates the ETC algorithm\n",
        "        m = int(T ** (2/3)/ (np.log(T))**(1/3))  #exploration threshold #Number of times explored(m) value based on Time Horizon\n",
        "        total_reward = 0  #cumulative reward\n",
        "\n",
        "        for n in range(T):\n",
        "            #deciding whether to explore or exploit\n",
        "            if n % m == 0:\n",
        "                action = np.random.randint(2)  #explore\n",
        "            else:\n",
        "                action = np.argmax([0.5, 0.5 + env.delta]) #exploit \n",
        "\n",
        "            _, reward, _, _ = env.step(action) \n",
        "            total_reward += reward #updating reward\n",
        "        regret = T *(0.5 + env.delta) - total_reward #diff between expected max reward and total obtained reward\n",
        "        return regret\n",
        "#parameters for mante carlo simulations  \n",
        "number_of_experiments = 500\n",
        "delta = 1/4\n",
        "T = 10000\n",
        "exploration_factor = 1.0 #we can take value between 0 and 1 \n",
        "# control balance (trafe off between between exploration and exploitation )\n",
        "#Monte Carlo simulations with etc algorithm\n",
        "total_regret = []\n",
        "\n",
        "for _ in range(number_of_experiments):\n",
        "    env = ArmedBanditsEnv(delta) #Create the environment \n",
        "    env.seed()\n",
        "    regret = etc_algorithm(env, T, exploration_factor)\n",
        "    total_regret.append(regret)\n",
        "\n",
        "avg_regret = np.mean(total_regret)\n",
        "\n",
        "print(f\"Estimated Expected Regret : {avg_regret}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### B2\n",
        "\n",
        "Repeat the above for various values of $\\Delta \\in \\{0.05, 0.1, 0.2, 0.3, 0.4, 0.45\\}$ and plot the estimated regret as a function of $\\Delta$ and verify whether it satisfies the regret upper bound we derived in class. `[5 Marks]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# write your code for the above part here\n",
        "# write your code for the above part here\n",
        "# Import necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "from gymnasium.utils import seeding\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class ArmedBanditsEnv(gym.Env):\n",
        "    def __init__(self, delta):\n",
        "        super(ArmedBanditsEnv, self).__init__()\n",
        "        #define action and observation space\n",
        "        self.delta = delta\n",
        "        self.action_space = spaces.Discrete(2)#two arms\n",
        "        self.observation_space = spaces.Discrete(1) #discrete choices between two options\n",
        "        self.true_means = [0.5, 0.5 + self.delta] #armed bandit parameters\n",
        "        self.step_count = 0 #tracking the current time step\n",
        "        self.seed_value = None\n",
        "        \n",
        "\n",
        "    def step(self, action):\n",
        "        #stimulate bandit step by taking action(choosing arm)\n",
        "        reward = np.random.binomial(1, self.true_means[action])\n",
        "        self.step_count += 1\n",
        "        done = self.step_count >= T #if time horizon is reached\n",
        "        return np.array([0.0]), reward, done, {}\n",
        "    \n",
        "    def reset(self):\n",
        "        self.step_count = 0 #reseting the environment to the initial state\n",
        "        return np.array([0.0])\n",
        "    \n",
        "    def render(self):\n",
        "        print(f\"Step: {self.step_count}, True Mreans: {self.true_means}\")\n",
        "\n",
        "    def seed(self, seed = None):\n",
        "        self.seed_value = seed\n",
        "        np.random.seed(seed)\n",
        "    \n",
        "    def close(self):\n",
        "        pass #cleanup and closing logic\n",
        "\n",
        "#ETC Algorithm\n",
        "def etc_algorithm(env, T, exploration_factor):\n",
        "        #simulates the ETC algorithm\n",
        "        m = int(T ** (2/3)/ (np.log(T))**(1/3)) #exploration threshold\n",
        "        #number of times each arm played based on time horizon\n",
        "        total_reward = 0 #cumulative reward\n",
        "\n",
        "        for n in range(T):\n",
        "            #deciding whether to explore or exploit\n",
        "            if n % m == 0:\n",
        "                action = np.random.randint(2) #explore\n",
        "            else:\n",
        "                action = np.argmax([0.5, 0.5 + env.delta]) #exploit\n",
        "\n",
        "            _, reward, _, _ = env.step(action)\n",
        "            total_reward += reward #updating reward\n",
        "        regret = T *(0.5 + env.delta) - total_reward #diff between expected max reward and total obtained reward\n",
        "        return regret\n",
        "#parameters   \n",
        "number_of_experiments = 500\n",
        "delta_values = [0.05,0.1,0.2,0.3,0.4,0.45]\n",
        "\n",
        "T = 10000\n",
        "exploration_factor = 1.0 #trade off between exploration and expoitation\n",
        "regret_results = []\n",
        "\n",
        "for delta in delta_values:\n",
        "    total_regret = 0\n",
        "    \n",
        "    for _ in range(number_of_experiments):\n",
        "        env = ArmedBanditsEnv(delta) #creating environment\n",
        "        env.seed()\n",
        "        regret = etc_algorithm(env, T, exploration_factor)\n",
        "        total_regret += regret\n",
        "\n",
        "    avg_regret = total_regret / number_of_experiments\n",
        "    regret_results.append(avg_regret)\n",
        "#plotting estimated regret\n",
        "plt.plot(delta_values, regret_results, marker= 'o')\n",
        "plt.xlabel('$\\Delta$') #latex model for empirical symbol\n",
        "plt.ylabel('Estimated Regret')\n",
        "plt.title('Estimated Regret as a function of $\\Delta$')\n",
        "#show the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### B3 \n",
        "Repeat the experiment with the UCB algorithm and plot the comparison with ETC. `[10 Marks]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Image size of 859x418963 pixels is too large. It must be less than 2^16 in each direction.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\shali\\anaconda3\\Lib\\site-packages\\IPython\\core\\formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m printer(obj)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
            "File \u001b[1;32mc:\\Users\\shali\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 152\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(bytes_io, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\shali\\anaconda3\\Lib\\site-packages\\matplotlib\\backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2363\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2364\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2365\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2366\u001b[0m         result \u001b[38;5;241m=\u001b[39m print_method(\n\u001b[0;32m   2367\u001b[0m             filename,\n\u001b[0;32m   2368\u001b[0m             facecolor\u001b[38;5;241m=\u001b[39mfacecolor,\n\u001b[0;32m   2369\u001b[0m             edgecolor\u001b[38;5;241m=\u001b[39medgecolor,\n\u001b[0;32m   2370\u001b[0m             orientation\u001b[38;5;241m=\u001b[39morientation,\n\u001b[0;32m   2371\u001b[0m             bbox_inches_restore\u001b[38;5;241m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2372\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2373\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
            "File \u001b[1;32mc:\\Users\\shali\\anaconda3\\Lib\\site-packages\\matplotlib\\backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2228\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m   2231\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[1;32m-> 2232\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: meth(\n\u001b[0;32m   2233\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m skip}))\n\u001b[0;32m   2234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
            "File \u001b[1;32mc:\\Users\\shali\\anaconda3\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_pil(filename_or_obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m\"\u001b[39m, pil_kwargs, metadata)\n",
            "File \u001b[1;32mc:\\Users\\shali\\anaconda3\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:457\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 457\u001b[0m     FigureCanvasAgg\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    458\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[0;32m    459\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    460\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
            "File \u001b[1;32mc:\\Users\\shali\\anaconda3\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:394\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;66;03m# docstring inherited\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_renderer()\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\shali\\anaconda3\\Lib\\site-packages\\matplotlib\\_api\\deprecation.py:384\u001b[0m, in \u001b[0;36mdelete_parameter.<locals>.wrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39minner_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_kwargs):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inner_args) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m name_idx \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m inner_kwargs:\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;66;03m# Early return in the simple, non-deprecated case (much faster than\u001b[39;00m\n\u001b[0;32m    383\u001b[0m         \u001b[38;5;66;03m# calling bind()).\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39minner_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_kwargs)\n\u001b[0;32m    385\u001b[0m     arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39minner_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_kwargs)\u001b[38;5;241m.\u001b[39marguments\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_varargs \u001b[38;5;129;01mand\u001b[39;00m arguments\u001b[38;5;241m.\u001b[39mget(name):\n",
            "File \u001b[1;32mc:\\Users\\shali\\anaconda3\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:411\u001b[0m, in \u001b[0;36mFigureCanvasAgg.get_renderer\u001b[1;34m(self, cleared)\u001b[0m\n\u001b[0;32m    409\u001b[0m reuse_renderer \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lastKey \u001b[38;5;241m==\u001b[39m key)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reuse_renderer:\n\u001b[1;32m--> 411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer \u001b[38;5;241m=\u001b[39m RendererAgg(w, h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi)\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lastKey \u001b[38;5;241m=\u001b[39m key\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cleared:\n",
            "File \u001b[1;32mc:\\Users\\shali\\anaconda3\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:84\u001b[0m, in \u001b[0;36mRendererAgg.__init__\u001b[1;34m(self, width, height, dpi)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m width\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m=\u001b[39m height\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renderer \u001b[38;5;241m=\u001b[39m _RendererAgg(\u001b[38;5;28mint\u001b[39m(width), \u001b[38;5;28mint\u001b[39m(height), dpi)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_renderers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_methods()\n",
            "\u001b[1;31mValueError\u001b[0m: Image size of 859x418963 pixels is too large. It must be less than 2^16 in each direction."
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# write your code for the above part here\n",
        "# write your code for the above part here\n",
        "# write your code for the above part here\n",
        "# Import necessary libraries\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "from gymnasium.utils import seeding\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class ArmedBanditsEnv(gym.Env):\n",
        "    def __init__(self, delta):\n",
        "        super(ArmedBanditsEnv, self).__init__()\n",
        "        #define action and observation space\n",
        "        self.delta = delta\n",
        "        self.action_space = spaces.Discrete(2) #two arms\n",
        "        self.observation_space = spaces.Discrete(1) #discrete choices between two options\n",
        "        self.true_means = [0.5, 0.5 + self.delta] #armed bandit parameters\n",
        "        self.step_count = 0 #tracking the current time step\n",
        "        self.seed_value = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.step_count = 0 #reseting the environment to the initial state\n",
        "        return np.array([0.0])\n",
        "    \n",
        "    def step(self, action):\n",
        "        #stimulate bandit step by taking action(choosing arm)\n",
        "        reward = np.random.binomial(1, self.true_means[action])\n",
        "        self.step_count += 1\n",
        "        done = self.step_count >= T #if time horizon is reached\n",
        "        return np.array([0.0]), reward, done, {}\n",
        "    \n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "    def seed(self, seed = None):\n",
        "        self.seed_value = seed\n",
        "        np.random.seed(seed)\n",
        "    \n",
        "    def close(self):\n",
        "        pass #cleanup and closing logic\n",
        " \n",
        "#ETC Algorithm\n",
        "def etc_algorithm(env, T, exploration_factor):\n",
        "        #simulates the ETC algorithm\n",
        "        m = int(T ** (2/3)/ (np.log(T))**(1/3)) #exploration thresold\n",
        "        total_reward = 0 \n",
        "        cumulative_regret = []\n",
        "\n",
        "        for n in range(T):\n",
        "            #deciding whether to explore or exploit\n",
        "            if n % m == 0:\n",
        "                action = np.random.randint(2) #exploring\n",
        "            else:\n",
        "                action = np.argmax([0.5, 0.5 + env.delta])  #exploiting\n",
        "\n",
        "            _, reward, _, _ = env.step(action)\n",
        "            total_reward += reward #updating reward\n",
        "            regret = T *(0.5 + env.delta) - total_reward #diff between expected max reward and total obtained reward\n",
        "            cumulative_regret.append(regret)\n",
        "        return cumulative_regret\n",
        "\n",
        "#UCB Algorithm\n",
        "def ucb_algorithm(env, T, c):\n",
        "    total_reward = 0 #cummulative\n",
        "    action_value = np.zeros(2) \n",
        "    action_counts = np.zeros(2) #number of times each arm has been explored \n",
        "    cumulative_regret = []  \n",
        "\n",
        "    for n in range(T):\n",
        "        exploration_bonus = c * np.sqrt(np.log(n + 1) /(2* action_counts))  #value of epsilon is squareroot of log(T)/n(a)\n",
        "        ucb_values = action_value + exploration_bonus\n",
        "        action = np.argmax(ucb_values)\n",
        "        _, reward, _, _ =env.step(action)\n",
        "        total_reward += reward\n",
        "        #updating action values and counts\n",
        "        action_value[action] += (reward - action_value[action]) / (action_counts[action] + 1)\n",
        "        action_counts[action] += 1\n",
        "        regret = T * (0.5 + env.delta) - total_reward\n",
        "        cumulative_regret.append(regret)\n",
        "    return cumulative_regret\n",
        "#parameters   \n",
        "#monte carlo simulations\n",
        "number_of_experiments = 500\n",
        "delta_values = [0.05,0.1,0.2,0.3,0.4,0.45]\n",
        "\n",
        "T = 10000\n",
        "exploration_factor = 1.0\n",
        "c = 2.0\n",
        "regret_results_etc = []\n",
        "regret_results_ucb = []\n",
        "\n",
        "\n",
        "for delta in delta_values:\n",
        "    total_regret_etc = 0\n",
        "    total_regret_ucb = 0\n",
        "    \n",
        "    for _ in range(number_of_experiments):\n",
        "        env_etc = ArmedBanditsEnv(delta)\n",
        "        env_etc.seed()\n",
        "        cumulative_regret_etc = etc_algorithm(env_etc, T, exploration_factor)\n",
        "        total_regret_etc += np.array(cumulative_regret_etc)\n",
        "\n",
        "        env_ucb = ArmedBanditsEnv(delta)\n",
        "        cumulative_regret_ucb = etc_algorithm(env_ucb, T, exploration_factor)\n",
        "        total_regret_ucb += np.array(cumulative_regret_ucb)\n",
        "\n",
        "\n",
        "    avg_regret_etc = total_regret_etc / number_of_experiments\n",
        "    regret_results_etc.append(avg_regret_etc)\n",
        "\n",
        "    avg_regret_ucb = total_regret_ucb / number_of_experiments\n",
        "    regret_results_ucb.append(avg_regret_ucb)\n",
        "\n",
        "\n",
        "#Plotting comparision betweeen ETC and UCB Algorithms as a function of delta\n",
        "plt.plot(delta_values, regret_results_etc, marker= 'o', label = 'ETC')\n",
        "plt.plot(delta_values, regret_results_ucb, marker= 'o', label = 'UCB')\n",
        "plt.xlabel('$\\Delta$') #latex mode for delta symbol\n",
        "plt.ylabel('Regret')\n",
        "plt.title('Comparision of ETC and UCB Algorithms')\n",
        "plt.legend()\n",
        "#show the figure\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### B4\n",
        " In the ETC algorithm, assume that we know $\\Delta$, and choose a better $m$ as function of $\\Delta$ and repeat the experiments and compare with UCB. What did you observe?\n",
        "\n",
        "Hint: Check how many samples of exploration are required to make $\\epsilon < \\frac{\\Delta}{2}$ with a high probability of $1-\\frac{1}{T}$. `[5 Marks]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHFCAYAAAAUpjivAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABuwElEQVR4nO3deVxU9f7H8dewLwIKCogr5Z6WpqVShqWi5lbdm5WaS8utTIvUNLNSq59bN/W2qC2m5Vrd3DXTSk1Tc0krtZtWZpogZQii7Hx/f4yMDIsCAgPD+/l4zAPOOd858/3OGeDN52wWY4xBRERExEm5OLoDIiIiIqVJYUdEREScmsKOiIiIODWFHREREXFqCjsiIiLi1BR2RERExKkp7IiIiIhTU9gRERERp6awIyIiIk5NYacC+f777xkyZAjh4eF4eXlRpUoVrr/+eqZNm8bff//t6O6VusGDB1O/fv0iPee3337DYrEwf/78UulTSfnwww+55ppr8Pb2xmKxsH///nzbbd68GYvFwn//+9+y7WAFVL9+fQYPHnzZdhaLpcDH4MGDbZ+hwjx+++03AH799VeGDRtGo0aN8Pb2xsfHh2uuuYbnnnuOP/74o3QHXgQdO3akY8eOl21nsVgYNmxYvsv++9//YrFY2Lx5c55lq1evplevXoSEhODh4UFgYCCdOnVi0aJFpKen260/58PX15emTZsyceJEzp07V6Qxvfbaa1gsFpo3b37J8UyYMKFI6y1Jud/38+fPM2HChHzfwwkTJmCxWPjrr7/KroNOyM3RHZDCeeeddxg6dCiNGzfm6aefplmzZqSnp7Nnzx7mzJnDjh07WL58uaO7Waqef/55nnzyySI9p2bNmuzYsYOrr766lHp15f7880/uv/9+unXrxqxZs/D09KRRo0aO7lal8s9//pORI0fmmV+jRg3bZyinoUOHkpCQwKJFi+zm16xZkzVr1nDvvfdSvXp1hg0bRqtWrbBYLPzwww+89957rF27ln379pXqeBzNGMMDDzzA/Pnzuf3225k+fTp16tQhISGBTZs2MXToUP766y+7n+ec2yApKYktW7bw4osv8v333/PJJ58U+rXfe+89AA4ePMg333xD27ZtS3ZwJWDWrFl20+fPn2fixIkAhQqfUnQKOxXAjh07eOyxx+jSpQsrVqzA09PTtqxLly6MHDmS9evXO7CHpev8+fP4+PgUK7B4enrSrl27UuhVyTl8+DDp6ekMGDCAyMhIR3enyLK3T0UWEhJyyc9J7mX+/v6kpaXlmX/06FHuvfdeGjVqxKZNmwgICLAtu+2223jiiSec/p8SgFdeeYX58+czceJEXnjhBbtlvXr1YvTo0fz8889283Nvg86dO3Ps2DEWLVpESkoKXl5el33dPXv28N1339GjRw/Wrl3L3Llzy1XYyf5ZadasmaO7UuloN1YFMGnSJCwWC2+//bZd0Mnm4eFB7969bdNZWVlMmzaNJk2a4OnpSXBwMAMHDuTEiRN2z+vYsSPNmzdnx44dRERE4O3tTf369Zk3bx4Aa9eu5frrr8fHx4cWLVrkCVTZ5dV9+/Zx11134e/vT0BAAAMGDODPP/+0a/vhhx8SFRVFzZo18fb2pmnTpjzzzDN5StSDBw+mSpUq/PDDD0RFReHn50enTp1sy3Lvxvr4449p27YtAQEB+Pj4cNVVV/HAAw/Ylhe0G2vbtm106tQJPz8/fHx8iIiIYO3atXZt5s+fj8ViYdOmTTz22GNUr16doKAg7rrrLk6ePJlnO+Rn1apVtG/fHh8fH/z8/OjSpYtdlWDw4MHcfPPNANxzzz1YLJYi/2eXvR2+//577r77bgICAggMDGTEiBFkZGTw008/0a1bN/z8/Khfvz7Tpk2ze372rrGFCxcyYsQIQkND8fb2JjIyMk8F4lLb5++//2bo0KHUqlULDw8PrrrqKsaNG0dqaqrt+a1ataJDhw55xpCZmUmtWrW46667bPPS0tJ4+eWXbZ/jGjVqMGTIkDyfrfT0dEaPHk1oaCg+Pj7cfPPN7Nq1q0jvYUmZPn06586dY9asWXZBJ5vFYrEbY35+/vlnhgwZQsOGDfHx8aFWrVr06tWLH374wa5d9nZbsmQJ48aNIywsDH9/fzp37sxPP/1k19YYw7Rp06hXrx5eXl5cf/31fPrpp1c+4Hykp6czdepUmjRpwvPPP59vm9DQUNvn/lICAgKwWCy4uroW6rXnzp0LwJQpU4iIiGDp0qWcP3++UM/dtm0b7du3x8vLi1q1avH888/z7rvv2u2ehKL/fv3qq6+IiIjAx8fH9rsp526s3377jRo1agAwceJEu12oOZ06dYr77ruPgIAAQkJCeOCBB0hISLBrk727cd68eTRu3Bhvb2/atGnDzp07McbwyiuvEB4eTpUqVbjtttvyBM59+/bRs2dPgoOD8fT0JCwsjB49euQZW4VkpFzLyMgwPj4+pm3btoV+zr/+9S8DmGHDhpn169ebOXPmmBo1apg6deqYP//809YuMjLSBAUFmcaNG5u5c+eazz77zPTs2dMAZuLEiaZFixZmyZIlZt26daZdu3bG09PT/PHHH7bnjx8/3gCmXr165umnnzafffaZmT59uvH19TWtWrUyaWlptrYvvfSSmTFjhlm7dq3ZvHmzmTNnjgkPDze33nqrXd8HDRpk3N3dTf369c3kyZPNF198YT777DPbsnr16tnabt++3VgsFnPvvfeadevWmS+//NLMmzfP3H///bY2R48eNYCZN2+ebd7mzZuNu7u7ad26tfnwww/NihUrTFRUlLFYLGbp0qW2dvPmzTOAueqqq8zw4cPNZ599Zt59911TrVq1PP3Oz6JFiwxgoqKizIoVK8yHH35oWrdubTw8PMzWrVuNMcb8/PPP5s033zSAmTRpktmxY4c5ePBggevctGmTAczHH3+cZzs0btzYvPTSS2bjxo1m9OjRts9AkyZNzGuvvWY2btxohgwZYgDzySef5FlnnTp1TJ8+fczq1avNwoULTYMGDYy/v7/55ZdfLrt9kpOTzbXXXmt8fX3Nv//9b7Nhwwbz/PPPGzc3N3P77bfbnv+f//zHAObw4cN241q3bp0BzKpVq4wxxmRmZppu3boZX19fM3HiRLNx40bz7rvvmlq1aplmzZqZ8+fP2/XJYrGYp59+2mzYsMFMnz7d1KpVy/j7+5tBgwZddjsBZujQoSY9PT3PIysrK9/nREZGmmuuuSbP/EaNGpmQkJDLvualbNmyxYwcOdL897//NVu2bDHLly83d9xxh/H29jb/+9//bO2yt1v9+vVN//79zdq1a82SJUtM3bp1TcOGDU1GRoatbfZn5MEHHzSffvqpefvtt02tWrVMaGioiYyMvGyfAPP444/nu+zjjz82gNm0aZMxxvpzCZgxY8YUesy5t0F8fLxZsWKF8fPzM/379y/UOs6fP28CAgLMDTfcYIwx5t133zWAmT9/fr6vN378eNv0d999Z7y8vMy1115rli5dalatWmVuv/12U79+fQOYo0eP2toW5fdrYGCgqVOnjnn99dfNpk2bzJYtW2zLst/3lJQUs379etv22bFjh9mxY4f5+eefjTH2P98vvPCC2bhxo5k+fbrx9PQ0Q4YMyTOuevXqmYiICLNs2TKzfPly06hRIxMYGGieeuop06dPH7NmzRqzaNEiExISYq699lrbZzwpKckEBQWZNm3amI8++shs2bLFfPjhh+bRRx81hw4dKtQ2KM8Udsq52NhYA5h77723UO1//PFH2y+OnL755hsDmGeffdY2LzIy0gBmz549tnmnT582rq6uxtvb2y7Y7N+/3wDmtddes83L/iF86qmn7F4r+4/8woUL8+1jVlaWSU9PN1u2bDGA+e6772zLBg0aZADz3nvv5Xle7rDz73//2wDmzJkzBb4f+YWddu3ameDgYHP27FnbvIyMDNO8eXNTu3Zt2w9/dtjJ/V5OmzbNACYmJqbA183MzDRhYWGmRYsWJjMz0zb/7NmzJjg42ERERNjm5RdgCnKpsPPqq6/atW3ZsqUBzLJly2zz0tPTTY0aNcxdd92VZ53XX3+93R/33377zbi7u5uHHnrINq+g7TNnzhwDmI8++shu/tSpUw1gNmzYYIwx5q+//jIeHh52n0NjjOnbt68JCQkx6enpxhhjlixZkieUGWPM7t27DWBmzZpljLn4eS/oM1jYsFPQY8GCBfk+p6Cw4+XlZdq1a3fZ1yyKjIwMk5aWZho2bGg3zuztljNMGmPMRx99ZACzY8cOY4wx8fHxxsvLy9x555127b7++msDlHjYWbp0qQHMnDlzCj3Ggt7/7t27m6SkpEKt44MPPrB73bNnz5oqVaqYDh065Pt6OcPO3XffbXx9fe3CSmZmpmnWrJld2CnO79cvvvgiz+vnDDvGGPPnn3/m6VO27J/vadOm2c0fOnSo8fLysvuZBUxoaKjde7ZixQoDmJYtW9q1nTlzpgHM999/b4wxZs+ePQYwK1asyNMHZ6DdWE5m06ZNAHlKoDfeeCNNmzbliy++sJtfs2ZNWrdubZsODAwkODiYli1bEhYWZpvftGlTAI4dO5bnNfv372833bdvX9zc3Gx9AevZKf369SM0NBRXV1fc3d1tx6f8+OOPedb5j3/847JjveGGG2yv99FHHxXqLJdz587xzTff8M9//pMqVarY5ru6unL//fdz4sSJPLsAcu4iBLj22muB/N+LbD/99BMnT57k/vvvx8Xl4o9ZlSpV+Mc//sHOnTsLXV4vrJ49e9pNN23aFIvFQvfu3W3z3NzcaNCgQb5979evHxaLxTZdr149IiIi7LZjttzb58svv8TX15d//vOfdvOzP4fZn7ugoCB69erF+++/T1ZWFgDx8fGsXLmSgQMH4uZmPYxwzZo1VK1alV69epGRkWF7tGzZktDQUNtZK9l9K+gzWFh9+/Zl9+7deR633357oddRUjIyMpg0aRLNmjXDw8MDNzc3PDw8OHLkSL4/K5f7fO7YsYOUlJQ871FERAT16tUrpVEUXc5t8NVXX/Haa6+xZ88eunXrZrcrtCBz587F29ube++9F7D+rN19991s3bqVI0eOXPK5W7Zs4bbbbqN69eq2eS4uLvTt29euXVF/v1arVo3bbrvtsn0vjPy2c0pKCnFxcXbzb731Vnx9fW3T2b+7u3fvbvfznft3eoMGDahWrRpjxoxhzpw5HDp0qET6XV4o7JRz1atXx8fHh6NHjxaq/enTpwFriMktLCzMtjxbYGBgnnbZp4jmngeQkpKSp31oaKjdtJubG0FBQbbXSkpKokOHDnzzzTe8/PLLbN68md27d7Ns2TIAkpOT7Z7v4+ODv7//JccJcMstt7BixQoyMjIYOHAgtWvXpnnz5ixZsqTA58THx2OMKfD9AfK8R0FBQXbT2cdN5e53TpfbDllZWcTHxxf4/OLIb5v5+PjkObDTw8OjUNsxe17u9yO/7XP69GlCQ0PtfpkCBAcH4+bmZreOBx54gD/++IONGzcCsGTJElJTU+3+gJw6dYozZ87g4eGBu7u73SM2NtZ2Gm72egv6DBZWjRo1aNOmTZ5Hfj8fl1K3bt1C/6wWZMSIETz//PPccccdrF69mm+++Ybdu3dz3XXX5fuZu9zns6D3qKB5+XF1dSUzMzPfZRkZGQC4u7sD1vcAKPL7kHMbdOjQgeHDh/Paa6+xbdu2y1464ueff+arr76iR48eGGM4c+YMZ86csYXv7DO0CnL69GlCQkLyzM89r6i/X/NrV1yF/T1U0O/uy/1ODwgIYMuWLbRs2ZJnn32Wa665hrCwMMaPH293mYCKSmGnnHN1daVTp07s3bu3UAeJZf9AxMTE5Fl28uRJu/9cSkpsbKzddEZGBqdPn7b15csvv+TkyZO89957PPTQQ9xyyy20adMGPz+/fNeX+w/mpfTp04cvvviChIQENm/eTO3atenXr1+eU4WzVatWDRcXlwLfH6BE3qPLbQcXFxeqVat2xa9TknJvx+x5uX/J5rd9goKCOHXqFMYYu/lxcXFkZGTYvaddu3YlLCzMdiD8vHnzaNu2rd0ZKtkHg+dXbdm9e7ft1N3svhX0GSxrXbt25dSpU+zcubPY61i4cCEDBw5k0qRJdO3alRtvvJE2bdoU+zorBb1HBc3LT0hISIGV0+z52cEgOySuXLkyz+ehqLKrVN99990l27333nsYY/jvf/9LtWrVbI8ePXoA8P777xcY1uDi5ze33O9PUX+/FuV3WXnQokULli5dyunTp9m/fz/33HMPL774Iq+++qqju3bFFHYqgLFjx2KM4eGHHyYtLS3P8vT0dFavXg1gK5kuXLjQrs3u3bv58ccfbWfOlKTc1xr56KOPyMjIsJ1tkP0Dn/tMsrfeeqvE+uDp6UlkZCRTp04FKPA6Jr6+vrRt25Zly5bZ/UeUlZXFwoULqV27dolc46Zx48bUqlWLxYsX2/3CP3fuHJ988ontDK3yZMmSJXZ9PXbsGNu3by/U2WGdOnUiKSmJFStW2M3/4IMPbMuzZe8yXLFiBVu3bmXPnj12Z9CBdZfc6dOnyczMzLfi0rhxY+DiNUkK+gyWtaeeegpfX1/bdXhyM8Zc9tRzi8WS52dl7dq1xb4YYbt27fDy8srzHm3fvv2Su2Jz6ty5M5s2bcpzJpwxho8//pj69evToEEDwFrhGTNmDP/73/946aWX8l1fXFwcX3/99WVfN/vimsHBwQW2yczM5P333+fqq69m06ZNeR4jR44kJibmkmefRUZG8uWXX9oFyqysLD7++GO7dqX1+7Uw1eKyZLFYuO6665gxYwZVq1bl22+/dXSXrpius1MBtG/fntmzZzN06FBat27NY489xjXXXEN6ejr79u3j7bffpnnz5vTq1YvGjRvzr3/9i9dffx0XFxe6d+/Ob7/9xvPPP0+dOnV46qmnSrx/y5Ytw83NjS5dunDw4EGef/55rrvuOtv+7oiICKpVq8ajjz7K+PHjcXd3Z9GiRZf9b+1yXnjhBU6cOEGnTp2oXbs2Z86c4T//+Y/d8UD5mTx5Ml26dOHWW29l1KhReHh4MGvWLA4cOMCSJUtK5L8xFxcXpk2bRv/+/enZsyePPPIIqampvPLKK5w5c4YpU6Zc8WuUtLi4OO68804efvhhEhISGD9+PF5eXowdO/ayzx04cCBvvvkmgwYN4rfffqNFixZs27aNSZMmcfvtt9O5c2e79g888ABTp06lX79+eHt7c88999gtv/fee1m0aBG33347Tz75JDfeeCPu7u6cOHGCTZs20adPH+68806aNm3KgAEDmDlzJu7u7nTu3JkDBw7w73//u1C7QrMVVI3x9/cv0jVRwsPDWbp0Kffccw8tW7a0XVQQ4NChQ7YKxJ133lngOnr27Mn8+fNp0qQJ1157LXv37uWVV16hdu3ahe5HTtWqVWPUqFG8/PLLPPTQQ9x9990cP36cCRMmFHo31gsvvMDq1atp27YtzzzzDA0bNiQ2NpZ33nmH3bt389FHH9m1f/rpp/nxxx8ZP348u3btol+/fraLCn711Ve8/fbbTJw4kZtuusn2nJzbICUlhf379/Pyyy9TtWpVhgwZUmDfPv30U06ePMnUqVPzDebNmzfnjTfeYO7cuXmOa8s2btw4Vq9eTadOnRg3bhze3t7MmTPHdmmM7OPuSuv3q5+fH/Xq1WPlypV06tSJwMBAqlevXuQrxl+JNWvWMGvWLO644w6uuuoqjDEsW7aMM2fO0KVLlzLrR6lxzHHRUhz79+83gwYNMnXr1jUeHh62U7xfeOEFExcXZ2uXmZlppk6daho1amTc3d1N9erVzYABA8zx48ft1lfQGSX16tUzPXr0yDOfXGdkZJ8lsHfvXtOrVy9TpUoV4+fnZ+677z5z6tQpu+du377dtG/f3vj4+JgaNWqYhx56yHz77bd5zpQaNGiQ8fX1zXf8uc/GWrNmjenevbupVauW8fDwMMHBweb222+3ndZtTP5nYxljzNatW81tt91mfH19jbe3t2nXrp1ZvXq1XZvss7F2795tNz/7LJjss08uZcWKFaZt27bGy8vL+Pr6mk6dOpmvv/463/Vd6dlYOc8kMabg9zL3ds9e54IFC8wTTzxhatSoYTw9PU2HDh3sztS71DqNsZ7J9+ijj5qaNWsaNzc3U69ePTN27FiTkpKSb/uIiAgDFHhqcXp6uvn3v/9trrvuOuPl5WWqVKlimjRpYh555BFz5MgRW7vU1FQzcuRIExwcbDsbaseOHaZevXpXfDbWTTfdlO9zCvrZyfbLL7+YoUOHmgYNGhhPT0/j7e1tmjVrZkaMGGF3GnN+4uPjzYMPPmiCg4ONj4+Pufnmm83WrVvznMFT0Ocmv898VlaWmTx5sqlTp47x8PAw1157rVm9enWedV7KkSNHzIABA2zbt2rVqiYqKirfs42yrVy50vTo0cPUqFHDuLm52S7bMGfOHJOammprl/t9d3d3N1dddZUZMmSI7RTsgtxxxx3Gw8PD7ndgbvfee69xc3MzsbGxttfLfebT1q1bTdu2bY2np6cJDQ01Tz/9tO1swpxnfF7p79fsZbnf988//9y0atXKeHp62p1JWNDPd/bvp5yfp9y/o425+Hl45ZVX7Obn/vz873//M/fdd5+5+uqrjbe3twkICDA33nhjvqfuV0QWY65wp6pUWhMmTGDixIn8+eefpXIskJSNzZs3c+utt/Lxxx/nOZtKpDKLiorit99+4/Dhw47uilwh7cYSEZFKb8SIEbRq1Yo6derw999/s2jRIjZu3Gi7KrNUbAo7IiJS6WVmZvLCCy8QGxuLxWKhWbNmLFiwgAEDBji6a1ICtBtLREREnJpOPRcRERGnprAjIiIiTk1hR0RERJyaDlDGeqXMkydP4ufnV+Eu7y0iIlJZGWM4e/YsYWFhdjddzk1hB+s9TerUqePoboiIiEgxHD9+/JJXGVfYAdsNKY8fP16kS8yLiIiI4yQmJlKnTp0CbyydTWGHizeq9Pf3V9gRERGpYC53CIoOUBYRERGnprAjIiIiTk1hR0RERJyajtkpgszMTNLT0x3dDamAPDw8LnlapIiIlB6FnUIwxhAbG8uZM2cc3RWpoFxcXAgPD8fDw8PRXRERqXQUdgohO+gEBwfj4+OjCw9KkWRftDImJoa6devq8yMiUsYUdi4jMzPTFnSCgoIc3R2poGrUqMHJkyfJyMjA3d3d0d0REalUdBDBZWQfo+Pj4+PgnkhFlr37KjMz08E9ERGpfBR2Ckm7HuRK6PMjIuI42o0lIiIipSMrE45th6RTUCUE6kWAi2uZd0NhR5zK4MGDOXPmDCtWrHB0V0REKrdDq2D9GEg8eXGefxh0mwrNepdpV7Qbq4xkZhl2/HKalfv/YMcvp8nMMqX6eoMHD+aOO+4o1deoiDZv3ozFYsn3ERsbS/369QtcbrFY6NixIwA///wzQ4YMoXbt2nh6ehIeHs59993Hnj17HDtAEZHy4NAq+GigfdABSIyxzj+0qky7o8pOGVh/IIaJqw8Rk5Bim1czwIvxvZrRrXlNB/as5GVmZmKxWMr9BfR++umnPDd9DQ4OZvfu3baDiLdv384//vEPu7YeHh7s2bOHTp060bx5c9566y2aNGnC2bNnWblyJSNHjmTLli1lPh4RkXIjK9Na0SG/f+oNYIH1z0CTHmW2S6t8/0VyAusPxPDYwm/tgg5AbEIKjy38lvUHYsqkHx07dmT48OFER0dTrVo1QkJCePvttzl37hxDhgzBz8+Pq6++mk8//dT2nOwqyNq1a7nuuuvw8vKibdu2/PDDD7Y28+fPp2rVqqxZs4ZmzZrh6enJsWPHiI+PZ+DAgVSrVg0fHx+6d+/OkSNHAEhISMDb25v169fb9XHZsmX4+vqSlJQEwB9//ME999xDtWrVCAoKok+fPvz222+29pmZmYwYMYKqVasSFBTE6NGjMaZwFbPg4GBCQ0PtHi4uLtSoUcM2HRgYmKdttWrVGDx4MA0bNmTr1q306NGDq6++mpYtWzJ+/HhWrlxZrO0jIuI0jm3PW9GxYyDxD2u7MqKwU0TGGM6nZRTqcTYlnfGrDhaYbQEmrDrE2ZT0Qq2vsH/IC/L+++9TvXp1du3axfDhw3nssce4++67iYiI4Ntvv6Vr167cf//9nD9/3u55Tz/9NP/+97/ZvXs3wcHB9O7d2+62GefPn2fy5Mm8++67HDx4kODgYAYPHsyePXtYtWoVO3bswBjD7bffTnp6OgEBAfTo0YNFixbZvc7ixYvp06cPVapU4fz589x6661UqVKFr776im3btlGlShW6detGWloaAK+++irvvfcec+fOZdu2bfz9998sX778it6jy9m/fz8HDx5k5MiR+VavqlatWqqvLyJS7iWdKtl2JUC7sYooOT2TZi98ViLrMkBsYgotJmwoVPtDL3bFx6P4m+y6667jueeeA2Ds2LFMmTKF6tWr8/DDDwPwwgsvMHv2bL7//nvatWtne9748ePp0qULYA1MtWvXZvny5fTt2xewXoto1qxZXHfddQAcOXKEVatW8fXXXxMREQHAokWLqFOnDitWrODuu++mf//+DBw4kPPnz+Pj40NiYiJr167lk08+AWDp0qW4uLjw7rvv2k7bnjdvHlWrVmXz5s1ERUUxc+ZMxo4dyz/+8Q8A5syZw2efFW7b1K5d2266Vq1a/PTTT5d9XnZ1qkmTJoV6HRGRSierkNcTqxJSuv3IQWGnErn22mtt37u6uhIUFESLFi1s80JCrB+8uLg4u+e1b9/e9n1gYCCNGzfmxx9/tM3z8PCwW/ePP/6Im5sbbdu2tc0LCgqye16PHj1wc3Nj1apV3HvvvXzyySf4+fkRFRUFwN69e/n555/x8/Oz60tKSgq//PILCQkJxMTE2PXNzc2NNm3aFKoCtnXrVrt1u7kV7kche926bo6ISC5ZWbDrLdg44TINLdazsupFlEWvAIWdIvN2d+XQi10L1XbX0b8ZPG/3ZdvNH3IDN4YHFuq1r0Tu2xRYLBa7edl/wLOysi67rpx/7L29ve2mCwobxhhbOw8PD/75z3+yePFi7r33XhYvXsw999xjCx1ZWVm0bt06z64usN564UqFh4cXa5dTo0aNAGuga9my5RX3Q0TEKZz+BVYOg98vHIcT3Azisv8pzvk34cLfim5TyvR6Ozpmp4gsFgs+Hm6FenRoWIOaAV4UVAOwYD0rq0PDGoVan6OqCTt37rR9Hx8fz+HDhy+5G6dZs2ZkZGTwzTff2OadPn2aw4cP07RpU9u8/v37s379eg4ePMimTZvo37+/bdn111/PkSNHCA4OpkGDBnaPgIAAAgICqFmzpl3fMjIy2Lt3b0kNO18tW7akWbNmvPrqq/mGwjNnzpTq64uIlCtZWbBzNsy+yRp03H2hx6vw6NfQ9wPwz3XGsX+YdX4ZX2dHlZ1S5OpiYXyvZjy28Fss5JttGd+rGa4u5XuXyIsvvkhQUBAhISGMGzeO6tWrX/IaPg0bNqRPnz48/PDDvPXWW/j5+fHMM89Qq1Yt+vTpY2sXGRlJSEgI/fv3p379+nbHCfXv359XXnmFPn368OKLL1K7dm1+//13li1bxtNPP03t2rV58sknmTJlCg0bNqRp06ZMnz690GEjLi6OlBT7M+SCgoIue5NOi8XCvHnz6Ny5M7fccgvPPvssTZo0ISkpidWrV7Nhwwadei4ilUPuak74LdD7DahWzzrdrLf19PJycAVlVXZKWbfmNZk94HpCA7zs5ocGeDF7wPUV4jo7U6ZM4cknn6R169bExMSwatUq240tCzJv3jxat25Nz549ad++PcYY1q1bl2e32X333cd3331nV9UB641Xv/rqK+rWrctdd91F06ZNeeCBB0hOTrZd82bkyJEMHDiQwYMH0759e/z8/LjzzjsLNabGjRtTs2ZNu0dhq0I33ngje/bs4eqrr+bhhx+madOm9O7dm4MHDzJz5sxCrUNEpMIqqJpz/8qLQSebiyuEd4AW/7R+dUDQAbCYKz2f2QkkJiYSEBBAQkJCngvNpaSkcPToUcLDw/Hy8ipgDZeXmWXYdfRv4s6mEOznxY3hgeW+orN582ZuvfVW4uPjdUr1FSqpz5GIiENdrppTxi719zsn7cYqI64uFtpfHeToboiIiBRd9plWn0+EjGRrNSfqRWj9AJTzK+aDwo6IiIhcSjmr5hSHwo4UqGPHjld81WYREamgKng1JyeFHREREbF3+hdY+Tj8vsM6XQGrOTkp7IiIiIhV7mqORxXo8iK0eQAq8JXjFXZERETE6ao5OSnsiIiIVGZOWs3JSWFHRESksnLiak5OCjsiIiKVTSWo5uSksCMiIlKZVJJqTk4OPVG+fv36WCyWPI/HH38cAGMMEyZMICwsDG9vbzp27MjBgwft1pGamsrw4cOpXr06vr6+9O7dmxMnTjhiOJeWlQlHt8IP/7V+zcos9Zfs2LEj0dHReeavWLHC7g7qaWlpTJs2jeuuuw4fHx+qV6/OTTfdxLx580hPTwdg8ODBdtsoKCiIbt268f3335f6OEREpATY3dNqh7Wa02M6DFzl1EEHHBx2du/eTUxMjO2xceNGAO6++24Apk2bxvTp03njjTfYvXs3oaGhdOnShbNnz9rWER0dzfLly1m6dCnbtm0jKSmJnj17kplZ+mGi0A6tgpnN4f2e8MmD1q8zm1vnO1haWhpdu3ZlypQp/Otf/2L79u3s2rWLxx9/nNdff90uXHbr1s22rb744gvc3Nzo2bOnA3svIiKFcvoXmH87rH/Gutsq/BZ4bDvc8KBT7rbKzaG7sWrUqGE3PWXKFK6++moiIyMxxjBz5kzGjRvHXXfdBcD7779PSEgIixcv5pFHHiEhIYG5c+eyYMECOnfuDMDChQupU6cOn3/+OV27di3zMeVxaBV8NBDIdSXixBjr/L4fQLPeDukawMyZM/nqq6/Ys2cPrVq1ss2/6qqruPvuu0lLS7PN8/T0JDQ0FIDQ0FDGjBnDLbfcwp9//plnW4qISDmQlQXfzIEvXqwUx+YUpNxc7zktLY2FCxfywAMPYLFYOHr0KLGxsURFRdnaeHp6EhkZyfbt1vtz7N27l/T0dLs2YWFhNG/e3NamxBkDaecK90hJhE9HkyfoWFdk/bJ+jLVdYdZXCrduWLRoEZ07d7YLOtnc3d3x9fXN93lJSUksWrSIBg0aEBSkG5yKiJQ72dWcz8ZWympOTuXmAOUVK1Zw5swZBg8eDEBsbCwAISEhdu1CQkI4duyYrY2HhwfVqlXL0yb7+flJTU0lNTXVNp2YmFj4jqafh0lhhW9/SQYST8KUOoVr/uxJ8Mg/fBTXkSNH6NixY6HarlmzhipVqgBw7tw5atasyZo1a3CpYPdIERFxavlVc6JegtZDKl3IyVZu/krNnTuX7t27ExZmHyQsuTaMMSbPvNwu12by5MkEBATYHnXqFDJsOKHCvJ/Zbr31Vvbv38/+/fv55ptviIqKonv37rbwKSIiDpanmhNpreZUst1WuZWLys6xY8f4/PPPWbZsmW1e9rEhsbGx1KxZ0zY/Li7OVu0JDQ0lLS2N+Ph4u+pOXFwcERERBb7e2LFjGTFihG06MTGx8IHH3cdaYSmMY9th0T8v367/f6Fewf21e+0i8Pf3JyEhIc/8M2fO4O/vD0CjRo348ccfC7U+X19fGjRoYJtu3bo1AQEBvPPOO7z88stF6puIiJQgVXMuqVxUdubNm0dwcDA9evSwzQsPDyc0NNR2hhZYj+vZsmWLLci0bt0ad3d3uzYxMTEcOHDgkmHH09MTf39/u0ehWSzWXUmFeVx9G/iHAQV90CzgX8varjDrK+IHtkmTJuzZsyfP/N27d9O4cWMA+vXrx+eff86+ffvytMvIyODcuXOXeCssuLi4kJycXKR+iYhICVI157IcHnaysrKYN28egwYNws3tYqHJYrEQHR3NpEmTWL58OQcOHGDw4MH4+PjQr18/AAICAnjwwQcZOXIkX3zxBfv27WPAgAG0aNHCdnaWQ7m4QrepFyZyf+AuTHebYm1XCoYOHcovv/zC448/znfffcfhw4d58803mTt3Lk8//TRgPXX/pptuolOnTrz55pt89913/Prrr3z00Ue0bduWI0eO2NaXmppKbGwssbGx/PjjjwwfPpykpCR69epVKv0XEZFLyMqCHbPsr5vTcwYMXOn0180pKofvxvr888/5/fffeeCBB/IsGz16NMnJyQwdOpT4+Hjatm3Lhg0b8PPzs7WZMWMGbm5u9O3bl+TkZDp16sT8+fNxdS2dAFFkzXpbTy9fP8Z6MHI2/zBr0CnF087r16/P1q1bGTduHFFRUaSkpNCoUSPmz59vu5aRp6cnGzduZMaMGbz11luMGjUKHx8fmjZtyhNPPEHz5s1t61u/fr1tl6Kfnx9NmjTh448/LvQBziIiUkLyXAU5Enq/rpBTAIsxpXA+cwWTmJhIQEAACQkJeXZppaSkcPToUcLDw/Hy8ir+i2RlWo/hSToFVUKsx+iUUkVHyp8S+xyJSOWmY3PsXOrvd04Or+xUGi6uEN7B0b0QEZGKStWcYlPYERERKc9UzbliCjsiIiLllao5JUJhR0RExFEKOp4zKzNHNSdF1ZwrpLBTSDqOW66EPj8iksehVfmfqXvTU3DgEzi+0zrvqo7Wak7Vug7ppjNQ2LkMd3d3AM6fP4+3t7eDeyMVVfbd48vNJRFExLEOrYKPBpLnRtGJJ+FT63XQVM0pOQo7l+Hq6krVqlWJi4sDwMfHp9D3khIB64Uz//zzT3x8fOwunCkilVRWprWikzvo5OTqCY9ug8DwMuuWM9Nv3kLIvk9XduARKSoXFxfq1q2roCwi1mN0Ei9zj8XMVEg4obBTQhR2CsFisVCzZk2Cg4NJT093dHekAvLw8MDFxeF3ZxGR8iDpVMm2k8tS2CkCV1dXHXMhIiJXxje4cO2qhJRuPyoRhR0REZGycvoX2PR/l2lksZ6VVS+iTLpUGSjsiIiIlLasLNj1Fnw+0XoVZDcv6/VzsGB/oPKF4/q6TdH9E0uQDiIQEREpTad/gfm3w/pnrEEnPBIe3wV9F4B/Tfu2/mHQ9wNo1tsxfXVSquyIiIiUhtzVHI8q0OVFaPOA9bo51epBkx75X0FZSpTCjoiISEk7/QusHAa/b7dOh98Cvd/Ie08rF1cI71D2/atkFHZERERKSlYW7HobPp9grea4+0LUi9D6AdDlJxxGYUdERKQk/P2rtZpz7GvrdEHVHClzCjsiIiJXIisLdr9jreakn1c1pxxS2BERESmu3NWc+h2gzxtQrb5DuyX2FHZERESKKr9qTpeJ0OZBVXPKIYUdERGRovj76IVqzjbrtKo55Z7CjoiISGFkZcHud+Hz8armVDAKOyIiIpeTXzWn9+sQGO7YfkmhKOyIiIgUJCsL9syFjeMh/Ry4+1y4CrKqORWJwo6IiEh+/j4Kq4bDb1ut0/Vuth6bo2pOhaOwIyIiklN+1ZzOE+GGh1TNqaAUdkRERLLF/2Y9NsdWzbnpQjXnKod2S66Mwo6IiEi+1ZwJcMPDquY4AYUdERGp3FTNcXoKOyIiUjllZcHe92DDC9Zqjpu3tZpz479UzXEyCjsiIlL5xB+DVcPg6FfW6boR1mpO0NWO7ZeUCoUdERGpPFTNqZQUdkREpHJQNafSUtgRERHnZgzseQ82vgBpSReqOePhxkdUzakkHL6V//jjDwYMGEBQUBA+Pj60bNmSvXv32pYbY5gwYQJhYWF4e3vTsWNHDh48aLeO1NRUhg8fTvXq1fH19aV3796cOHGirIciIiLlzZnf4YM+sHaENejUbQ+PfQ3tHlPQqUQcuqXj4+O56aabcHd359NPP+XQoUO8+uqrVK1a1dZm2rRpTJ8+nTfeeIPdu3cTGhpKly5dOHv2rK1NdHQ0y5cvZ+nSpWzbto2kpCR69uxJZmamA0YlIiIOl13NmdUejm6xVnO6TobB67TbqhKyGGOMo178mWee4euvv2br1q35LjfGEBYWRnR0NGPGjAGsVZyQkBCmTp3KI488QkJCAjVq1GDBggXcc889AJw8eZI6deqwbt06unbtetl+JCYmEhAQQEJCAv7+/iU3QBERKXtnfrfe0+rXzdbpOu3gjlkKOU6osH+/HVrZWbVqFW3atOHuu+8mODiYVq1a8c4779iWHz16lNjYWKKiomzzPD09iYyMZPv27QDs3buX9PR0uzZhYWE0b97c1kZERCqBnNWcXzdfrOYMUTWnsnPoAcq//vors2fPZsSIETz77LPs2rWLJ554Ak9PTwYOHEhsbCwAISEhds8LCQnh2LFjAMTGxuLh4UG1atXytMl+fm6pqamkpqbaphMTE0tyWCIiUtZUzZFLcGjYycrKok2bNkyaNAmAVq1acfDgQWbPns3AgQNt7SwWi93zjDF55uV2qTaTJ09m4sSJV9h7ERFxOGNg73zY8DyknQU3L+j0ArR9FFxcHd07KSccuhurZs2aNGvWzG5e06ZN+f333wEIDQ0FyFOhiYuLs1V7QkNDSUtLIz4+vsA2uY0dO5aEhATb4/jx4yUyHhERKUNnjsOCO2FNtDXo1GkLj34N7R9X0BE7Dg07N910Ez/99JPdvMOHD1OvXj0AwsPDCQ0NZePGjbblaWlpbNmyhYiICABat26Nu7u7XZuYmBgOHDhga5Obp6cn/v7+dg8REakgsqs5s9rDr5us1Zyo/4Mhn0L1Bo7unZRDDt2N9dRTTxEREcGkSZPo27cvu3bt4u233+btt98GrLuvoqOjmTRpEg0bNqRhw4ZMmjQJHx8f+vXrB0BAQAAPPvggI0eOJCgoiMDAQEaNGkWLFi3o3LmzI4cnIiIl7cxxWP0E/PKldbpOW+jzJlRv6Nh+Sbnm0LBzww03sHz5csaOHcuLL75IeHg4M2fOpH///rY2o0ePJjk5maFDhxIfH0/btm3ZsGEDfn5+tjYzZszAzc2Nvn37kpycTKdOnZg/fz6uripjiog4BWPg2/fhs+cuHptz2/MXLg6o3/VyaQ69zk55oevsiIiUY7mrObVvtJ5ppWpOpVfYv9+6N5aIiJRPxsC3H8Bn43JUc56DdkNVzZEiUdgREZHyJ+EErHoCfvnCOq1qjlwBhR0RESk/jIF9C6zVnNREcPW0VnN0OrlcAYUdEREpHxJOwOon4efPrdO1b4A+s6BGI8f2Syo8hR0REXEsVXOklCnsiIiI4yT8YT3TStUcKUUKOyIiUvaMgX0L4bNnc1RzxkH7YarmSIlT2BERkbKV8MeFY3Mu3OanVhvrmVY1Gju2X+K0FHZERKRsGAP7F8H6ZyE1wVrNufVZiBiuao6UKoUdEREpfYknrdWcIxus07Vawx2zVc2RMqGwIyIipccY2L8Y1o+1r+a0Hwau+hMkZUOfNBERKR35VXP6zILgJo7tl1Q6CjsiIlKyjIHvlsCnz1yo5nhcqOYMVzVHHEKfOhERKTmJJ2F1NBz5zDoddr312BxVc8SBFHZEROTKZVdz1j8DKReqOR3HQsQTquaIw+kTKCIiVyYx5sKxOTmrObMguKlj+yVygcKOiIgUjzHw3VJYP0bVHCnX9GkUEZGiS4yBNdFweL11OqzVhWNzVM2R8kdhR0RECs8Y+P5D+HR0jmrOMxDxpKo5Um7pkykiIoWTXzWnzywIaebQbolcjsKOiIhcWu5qjou7tZpzU7SqOVIh6FMqIiIFOxtrvW7O4U+t0zVbWo/NUTVHKhCFHRERycsY+P6jC9WcM6rmSIWmT6yIiNg7GwtrnoKf1lmnVc2RCk5hR0RErIyBHz6GdU/nqOaMuVDNcXd070SKTWFHRETg7CnrmVa2as51F6o51zi0WyIlQWFHRKQyy6+aEzkGbo5WNUechsKOiEhldfbUhWNz1lqnQ6+1VnNCmzu2XyIlTGFHRKSyMQZ++C98+jQkx1+o5oyGm59SNUecksKOiEhlcvYUrB0B/1tjnVY1RyoBhR0RkcrAGDjwCawbpWqOVDoKOyIizi4pznpsjq2a0wLumKNqjlQaCjsiIs4qTzXHDW4ZDR1GqJojlYrCjoiIM8q3mjPb+lWkknFx5ItPmDABi8Vi9wgNDbUtN8YwYcIEwsLC8Pb2pmPHjhw8eNBuHampqQwfPpzq1avj6+tL7969OXHiRFkPRUSk7GVlwtGt1jOrjm61TmefafVmW2vQcXGDjmPh4U0KOlJpObyyc8011/D555/bpl1dXW3fT5s2jenTpzN//nwaNWrEyy+/TJcuXfjpp5/w8/MDIDo6mtWrV7N06VKCgoIYOXIkPXv2ZO/evXbrEhFxKodWwfoxkHjy4rwqoRBQG/7YY51WNUcEKAdhx83Nza6ak80Yw8yZMxk3bhx33XUXAO+//z4hISEsXryYRx55hISEBObOncuCBQvo3LkzAAsXLqROnTp8/vnndO3atUzHIiJSJg6tgo8GAsZ+flKs9WFxuXAV5BHg5uGQLoqUJw7djQVw5MgRwsLCCA8P59577+XXX38F4OjRo8TGxhIVFWVr6+npSWRkJNu3bwdg7969pKen27UJCwujefPmtjb5SU1NJTEx0e4hIlIhZGVaKzq5g05OPtXhlqcVdEQucGjYadu2LR988AGfffYZ77zzDrGxsURERHD69GliY2MBCAkJsXtOSEiIbVlsbCweHh5Uq1atwDb5mTx5MgEBAbZHnTp1SnhkIiKl5Nh2+11X+TkXZ20nIoCDw0737t35xz/+QYsWLejcuTNr11rvz/L+++/b2lgsFrvnGGPyzMvtcm3Gjh1LQkKC7XH8+PErGIWISBlKOlWy7UQqAYfvxsrJ19eXFi1acOTIEdtxPLkrNHFxcbZqT2hoKGlpacTHxxfYJj+enp74+/vbPUREyr2sLPjj28K1rVLw70CRyqZchZ3U1FR+/PFHatasSXh4OKGhoWzcuNG2PC0tjS1bthAREQFA69atcXd3t2sTExPDgQMHbG1ERJzC37/CB71h55uXaWgB/1pQT78DRbI59GysUaNG0atXL+rWrUtcXBwvv/wyiYmJDBo0CIvFQnR0NJMmTaJhw4Y0bNiQSZMm4ePjQ79+/QAICAjgwQcfZOTIkQQFBREYGMioUaNsu8VERCq8rEzYORu+fBkyksHNG5rfBfsXX2iQ80DlC7vvu00BF116QySbQ8POiRMnuO+++/jrr7+oUaMG7dq1Y+fOndSrVw+A0aNHk5yczNChQ4mPj6dt27Zs2LDBdo0dgBkzZuDm5kbfvn1JTk6mU6dOzJ8/X9fYEZGKL+5HWDns4nVzwm+BXq9BYDg06pb3Ojv+Ydag06y3Y/orUk5ZjDGXOH+xckhMTCQgIICEhAQdvyMijpeZDttmwFevQGYaePpD1Etw/SDIefJFVqb1rKukU9ZjdOpFqKIjlUph/347/KKCIiKSw8l91mrOqQPW6UbdoMd0CKiVt62LK4R3KNv+iVRACjsiIuVBejJsngLbXweTCd6B0H0atPinfTVHRIpMYUdExNGO7YBVw+D0z9bpa+6yBp0qNRzbLxEnobAjIuIoqUnwxUTY9Q5grDfy7DkdmvRwdM9EnIrCjoiII/zyJax6EhJ+t063GgBRL4N3tUs/T0SKTGFHRKQsJcfDZ8/B/oXW6YC60Ps/cPVtju2XiBNT2BERKSs/roG1Iy7ct8oCN/4LOr0AnlUc3TMRp6awIyJS2pL+hE+fhoPLrdNBDaHPG1C3nWP7JVJJKOyIiJQWY+CHj+HTMZD8N1hc4aYnIPIZcPdydO9EKg2FHRGR0pDwB6x5Co58Zp0OaWGt5oS1dGi3RCojhR0RkZJkDOydDxtfgNREcPWAW0bDzdHg6u7o3olUSgo7IiIl5e9fYdUT8NtW63StNtDnTQhu4th+iVRyCjsiIlcqKxO+mQNfvAQZyeDmDZ2eh7aP6sacIuWAwo6IyJWI+5/1Vg8ndlun63eA3q9B4FWO7ZeI2CjsiIgUR2Y6bJsJX02DzDTw9Ieol+D6Qbpxp0g5o7AjIlJUJ/fDymFw6gfrdMOu0HMGBNRyaLdEJH8KOyIihZWeAlumwNevgckE70Dr3clb/FPVHJFyTGFHRKQwju2wHptz+mfr9DV3WYNOlRqO7ZeIXJbCjojIpaQmwRcTYdc7gIEqodDjVWja09E9E5FCUtgRESnIL1/Cqich4XfrdKsBEPUyeFdzbL9EpEgUdkREckuOh8+eg/0LrdMBdaH3f+Dq2xzbLxEpFoUdEZGcflwDa0dCUixggRv/BZ1eAM8qju6ZiBSTwo6ICEDSn/Dp03BwuXU6qCH0fh3qtXdsv0TkiinsiEjlZgz88DF8OgaS/waLK9z0BEQ+A+5eju6diJQAhR0RqbwS/oA1T8GRz6zTIS2gz+sQ1sqx/RKREqWwIyKVjzGwdz5sfAFSE8HVA24ZDTdHg6u7o3snIiVMYUdEKpe/f4VVT8BvW63TtdpAnzchuIlj+yUipUZhR0Qqh6xM+GYOfPESZCSDmzd0eh7aPgouro7unYiUIoUdEXF+cf+z3urhxG7rdP0O0Ps1CLzKsf0SkTLhUpwnPfDAA5w9ezbP/HPnzvHAAw9ccadEREpEZjpseQXe6mANOh5+0HMmDFqtoCNSiViMMaaoT3J1dSUmJobg4GC7+X/99RehoaFkZGSUWAfLQmJiIgEBASQkJODv7+/o7ohISTi5H1YOg1M/WKcbdoWeMyCglkO7JSIlp7B/v4u0GysxMRFjDMYYzp49i5fXxWtQZGZmsm7dujwBSESkTKWnwJYp8PVrYDLBOxC6T4UWd4PF4ujeiYgDFCnsVK1aFYvFgsVioVGjRnmWWywWJk6cWGKdExEpkt93Wqs5p49Yp6+5E7q/AlVqOLZfIuJQRQo7mzZtwhjDbbfdxieffEJgYKBtmYeHB/Xq1SMsLKzEOykickmpSfDFi7DrbcBAlRDoMR2a9nR0z0SkHCjSAcqRkZF07NiRo0ePcscddxAZGWl7tG/f/oqCzuTJk7FYLERHR9vmGWOYMGECYWFheHt707FjRw4ePGj3vNTUVIYPH0716tXx9fWld+/enDhxotj9EJEK5pcvYXZ72PUWYKDlAHj8GwUdEbEp1tlY9erVY9u2bQwYMICIiAj++OMPABYsWMC2bduKvL7du3fz9ttvc+2119rNnzZtGtOnT+eNN95g9+7dhIaG0qVLF7szwaKjo1m+fDlLly5l27ZtJCUl0bNnTzIzM4szNBGpKJLPwMrHYcGdcOZ3CKgLA5bBHW+CdzVH905EypFihZ1PPvmErl274u3tzbfffktqaioAZ8+eZdKkSUVaV1JSEv379+edd96hWrWLv6CMMcycOZNx48Zx11130bx5c95//33Onz/P4sWLAUhISGDu3Lm8+uqrdO7cmVatWrFw4UJ++OEHPv/88+IMTUQqgv+thTfbwr6FgAVufASG7oAGnRzdMxEph4oVdl5++WXmzJnDO++8g7v7xfvIRERE8O233xZpXY8//jg9evSgc+fOdvOPHj1KbGwsUVFRtnmenp5ERkayfft2APbu3Ut6erpdm7CwMJo3b25rk5/U1FQSExPtHiJSAST9CR8PgaX9ICkWghrAkE/h9mngWcXRvRORcqpYV1D+6aefuOWWW/LM9/f358yZM4Vez9KlS/n222/ZvXt3nmWxsbEAhISE2M0PCQnh2LFjtjYeHh52FaHsNtnPz8/kyZN11phIRWIM/PBf+HQ0JP8NFle46QmIHAPu3o7unYiUc8Wq7NSsWZOff/45z/xt27Zx1VWFuyrp8ePHefLJJ1m4cKHd9Xpys+S6LoYxJs+83C7XZuzYsSQkJNgex48fL1SfRcQBEv6AJffCsoesQSekOTz8BXSeoKAjIoVSrLDzyCOP8OSTT/LNN99gsVg4efIkixYtYtSoUQwdOrRQ69i7dy9xcXG0bt0aNzc33Nzc2LJlC6+99hpubm62ik7uCk1cXJxtWWhoKGlpacTHxxfYJj+enp74+/vbPUSknDEG9s6HWe3g8Hpw9YBbn4OHN0FYK0f3TkQqkGLtxho9ejQJCQnceuutpKSkcMstt+Dp6cmoUaMYNmxYodbRqVMnfvjhB7t5Q4YMoUmTJowZM4arrrqK0NBQNm7cSKtW1l9saWlpbNmyhalTpwLQunVr3N3d2bhxI3379gUgJiaGAwcOMG3atOIMTUTKg79/hVVPwG9brdO12kCfNyG4iWP7JSIVUpHDTmZmJtu2bWPkyJGMGzeOQ4cOkZWVRbNmzahSpfAHCPr5+dG8eXO7eb6+vgQFBdnmR0dHM2nSJBo2bEjDhg2ZNGkSPj4+9OvXD4CAgAAefPBBRo4cSVBQEIGBgYwaNYoWLVrkOeBZRCqArEz4Zg588RJkJIObN3R6Hto+Ci6uju6diFRQRQ47rq6udO3alR9//JHAwEDatGlTGv0CrBWk5ORkhg4dSnx8PG3btmXDhg34+fnZ2syYMQM3Nzf69u1LcnIynTp1Yv78+bi66hejSIUS9z9YNcx6d3KA+h2g92u6O7mIXLFi3fX8hhtuYMqUKXTq5BzXtNBdz0UcKDMdts2Er6ZBZhp4+EHUS3D9IHAp1mGFIlJJlMpdz7P93//9H6NGjeKll16idevW+Pr62i1XYBCRQjm533rjzlMXjt9r2BV6zoCAWg7tlog4l2JVdlxy/LeV8xTv7FO+K9qtGlTZESlj6SmwZQp8/RqYTPAOhO5TocXdcJlLS4iIZCvVys6mTZuK3TERqeR+32mt5pw+Yp2+5k7o/gpUqeHYfomI0ypW2ImMjCzpfoiIs0tNgi9ehF1vAwaqhECP6bo7uYiUumKFne+//z7f+RaLBS8vL+rWrYunp+cVdUxEnMgvX8LqJ613JwdoOQC6vqy7k4tImShW2GnZsuUlb8fg7u7OPffcw1tvvXXJW0GIiJNLPgMbxl24OzkQUBd6zdTdyUWkTBXrvM7ly5fTsGFD3n77bfbv38++fft4++23ady4MYsXL2bu3Ll8+eWXPPfccyXdXxGpKP63Ft5sezHo3PgvGLpDQUdEylyxTz3/z3/+Q9euXW3zrr32WmrXrs3zzz/Prl278PX1ZeTIkfz73/8usc6KSAVw7i9Y9zQcXGadDmoAvd+Aeu0d2y8RqbSKFXZ++OEH6tWrl2d+vXr1bPe7atmyJTExMVfWOxGpOIyBH/4Ln4623p3c4goRw6HjM7o7uYg4VLF2YzVp0oQpU6aQlpZmm5eens6UKVNo0sR6o74//vjjknceFxEnkngSltwLyx6yBp2Q5vDwF9BlooKOiDhcsSo7b775Jr1796Z27dpce+21WCwWvv/+ezIzM1mzZg0Av/76K0OHDi3RzopIOWMMfPs+bHgeUhPBxR0iR8NN0eDm4ejeiYgAxbyCMkBSUhILFy7k8OHDGGNo0qQJ/fr1s7tJZ0WhKyiLFMPfR2H1E3D0K+t0rTbQ5w0IburYfolIpVGqV1AGqFKlCo8++mhxny4iFVVWJnzzFnz5EqSfBzdvuO05aPcYuLg6unciInkU+5bCCxYs4OabbyYsLIxjx44BMGPGDFauXFlinRORcubPn+C9rvDZWGvQqd8BHvsaIoYp6IhIuVWssDN79mxGjBhB9+7diY+Pt934s1q1asycObMk+yci5UFmOnz1Csy5GU7sBg8/6DkTBq6CoKsd3TsRkUsqVth5/fXXeeeddxg3bhxubhf3hLVp08Z26rmIOImY7+CdW+HLlyEzDRpGweM7oc0QcCl2cVhEpMwU65ido0eP0qpVqzzzPT09OXfu3BV3SkTKgfQU2DIVvv4PmEzrfay6T4MWd8MlbhcjIlLeFCvshIeHs3///jwXFvz0009p2lRnYohUGFmZcGw7JJ2y3oW8XoT12Jvfd8LKYXD6iLXdNXdC91egSg3H9ldEpBiKFXaefvppHn/8cVJSUjDGsGvXLpYsWcKkSZOYO3duSfdRRErDoVWwfoz1goDZ/GpCaAs4shEw1gDUYzo07emwboqIXKlihZ0hQ4aQkZHB6NGjOX/+PP369aNWrVq8/vrrdOjQoaT7KCIl7dAq+GggkOsyW2djrA+AlgOg68vW3VciIhVYsY8ufPjhhzl27BhxcXHExsaya9cu9u3bR4MGDUqyfyJS0rIyrRWd3EEnJ58g6P2ago6IOIUihZ0zZ87Qv39/atSoQVhYGK+99hqBgYG8+eabNGjQgJ07d/Lee++VVl9FpCQc226/6yo/509b24mIOIEi7cZ69tln+eqrrxg0aBDr16/nqaeeYv369aSkpLBu3ToiIyNLq58iUlKSTpVsOxGRcq5IYWft2rXMmzePzp07M3ToUBo0aECjRo10IUGRiiT+WOHaVQkp3X6IiJSRIoWdkydP0qxZMwCuuuoqvLy8eOihh0qlYyJSwpLjYf1Y+G7JZRpawD/Mehq6iIgTKNIxO1lZWbi7u9umXV1d8fX1LfFOiUgJ+2k9vNnuQtCxQOPu1q/kvjjgheluU3SvKxFxGkWq7BhjGDx4MJ6engCkpKTw6KOP5gk8y5YtK7keikjx5a7mBDWAO2ZDnRvzv86Of5g16DTr7Zj+ioiUgiKFnUGDBtlNDxgwoEQ7IyIl6PBnsPrJC9fNsVjvTH7rOHD3ti5v1hua9Mj/CsoiIk7EYoy5xMU2KofExEQCAgJISEjA39/f0d0RuTLJ8bD+WfhusXU6ZzVHRMSJFPbvd7GuoCwi5VTuak77x+G25y5Wc0REKiGFHRFnkHzmwrE5Oao5fWZB3bYO7ZaISHmgsCNS0R3eAKufUDVHRKQACjsiFVXyGfjsWdi/yDqtao6ISL4UdkQqIlVzREQKrdh3PS8Js2fP5tprr8Xf3x9/f3/at2/Pp59+altujGHChAmEhYXh7e1Nx44dOXjwoN06UlNTGT58ONWrV8fX15fevXtz4sSJsh6KSNlIPgMrhsLiu61BJ/BqeGA9dP0/BR0RkQI4NOzUrl2bKVOmsGfPHvbs2cNtt91Gnz59bIFm2rRpTJ8+nTfeeIPdu3cTGhpKly5dOHv2rG0d0dHRLF++nKVLl7Jt2zaSkpLo2bMnmZmZjhqWSOk4shFmtb+w28oC7YfBo9ugbjtH90xEpFwrd9fZCQwM5JVXXuGBBx4gLCyM6OhoxowZA1irOCEhIUydOpVHHnmEhIQEatSowYIFC7jnnnsA6/276tSpw7p16+jatWuhXlPX2ZFyLfkMfDYO9i+0TgdeDXfMUsgRkUqvsH+/HVrZySkzM5OlS5dy7tw52rdvz9GjR4mNjSUqKsrWxtPTk8jISLZv3w7A3r17SU9Pt2sTFhZG8+bNbW3yk5qaSmJiot1DpFyyVXMWAhZo97iqOSIiReTwA5R/+OEH2rdvT0pKClWqVGH58uU0a9bMFlZCQkLs2oeEhHDs2DEAYmNj8fDwoFq1annaxMbGFviakydPZuLEiSU8EpESlF81p8+bUK+9Q7slIlIRObyy07hxY/bv38/OnTt57LHHGDRoEIcOHbItt1js78psjMkzL7fLtRk7diwJCQm2x/Hjx69sECIlqaBqjoKOiEixOLyy4+HhQYMGDQBo06YNu3fv5j//+Y/tOJ3Y2Fhq1qxpax8XF2er9oSGhpKWlkZ8fLxddScuLo6IiIgCX9PT09N253aRciP5DGwYB/uyqzlXWa+bo5AjInJFHF7Zyc0YQ2pqKuHh4YSGhrJx40bbsrS0NLZs2WILMq1bt8bd3d2uTUxMDAcOHLhk2BEpd458bq3m7Muu5gyFR79W0BERKQEOrew8++yzdO/enTp16nD27FmWLl3K5s2bWb9+PRaLhejoaCZNmkTDhg1p2LAhkyZNwsfHh379+gEQEBDAgw8+yMiRIwkKCiIwMJBRo0bRokULOnfu7MihiRROSoL1Ksh21Zw3oZ7CuohISXFo2Dl16hT3338/MTExBAQEcO2117J+/Xq6dOkCwOjRo0lOTmbo0KHEx8fTtm1bNmzYgJ+fn20dM2bMwM3Njb59+5KcnEynTp2YP38+rq6ujhqWSOH8/DmsegIS/8BazXkMbnsePHwc3TMREadS7q6z4wi6zo6UqZQE65lW+xZYp1XNEREplsL+/Xb4AcoilUruak7bR6HTC6rmiIiUIoUdkbKQu5pTLdx6FWRVc0RESp3CjkhpUzVHRMShFHZESktKAmx4Dr79wDpdLdx6bE79mxzbLxGRSkZhR6Q0/PwFrBp+oZpDjmqOr2P7JSJSCSnsiJSklETrVZBt1Zz61qsgq5ojIuIwCjsiJeXnLy4cm3PCOq1qjohIuaCwI3KlUhIvHJvzvnVa1RwRkXJFYUfkSuSu5tz4CHQer2qOiEg5orAjUhz5VnPehPo3O7RbIiKSl8KOSFH98qW1mpNw3Dqtao6ISLmmsCNSWCmJsPF52DvfOq1qjohIhaCwI1IYeao5/4LOE1TNERGpABR2RC4ldzWnaj1rNSe8g0O7JSIihaewI1KQXzZZr4Kcs5rTaTx4VnFsv0REpEgUdkRySz0LG56HvfOs06rmiIhUaAo7Ijnlrubc8LD12BxVc0REKiyFHREooJrzBoTf4th+iYjIFVPYEfl1M6wcDgm/W6dVzRERcSoKO1J5pZ6FjS/Anves01XrXjg2R9UcERFnorAjldOvW2DlsBzVnIeg80RVc0REnJDCjlQuquaIiFQ6CjtSeaiaIyJSKSnsiPNLTbpQzZlrna5aF3q/AVdFOrZfIiJSJhR2xLn9ugVWDYMzF6o5bR6ELhPB08+x/RIRkTKjsCPOKXc1J6Cu9bo5quaIiFQ6CjvifI5+BSsfVzVHREQAhR1xJqlJ8Pl42P2udTqgLvR5Ha7q6NBuiYiIYynsiHM4+pX1TKszx6zTbR6ALi+qmiMiIgo7UsGpmiMiIpehsCMV19GtF47NUTVHREQKprAjFU9qEnw+AXa/Y50OqHPhTKuOjuyViIiUUwo7UrHkrua0HgJRL6maIyIiBXJx5ItPnjyZG264AT8/P4KDg7njjjv46aef7NoYY5gwYQJhYWF4e3vTsWNHDh48aNcmNTWV4cOHU716dXx9fenduzcnTpwoy6FIaUtNgrWj4P2e1qATUAfuXwG9ZiroiIjIJTk07GzZsoXHH3+cnTt3snHjRjIyMoiKiuLcuXO2NtOmTWP69Om88cYb7N69m9DQULp06cLZs2dtbaKjo1m+fDlLly5l27ZtJCUl0bNnTzIzMx0xLClpR7fC7IiLu61aD4GhO+DqWx3bLxERqRAsxhjj6E5k+/PPPwkODmbLli3ccsstGGMICwsjOjqaMWPGANYqTkhICFOnTuWRRx4hISGBGjVqsGDBAu655x4ATp48SZ06dVi3bh1du3a97OsmJiYSEBBAQkIC/v7+pTpGKYK0c9Zjc3a9bZ0OqAO9X1fIERERoPB/vx1a2cktISEBgMDAQACOHj1KbGwsUVFRtjaenp5ERkayfft2APbu3Ut6erpdm7CwMJo3b25rIxXQb9us1ZzsoNN6MDy2XUFHRESKrNwcoGyMYcSIEdx88800b94cgNjYWABCQkLs2oaEhHDs2DFbGw8PD6pVq5anTfbzc0tNTSU1NdU2nZiYWGLjkCuUu5rjX9t63Zyrb3Not0REpOIqN2Fn2LBhfP/992zbti3PMovFYjdtjMkzL7dLtZk8eTITJ04sfmeldPy2zXqmVfxv1unWg6HLS+ClXYsiIlJ85WI31vDhw1m1ahWbNm2idu3atvmhoaEAeSo0cXFxtmpPaGgoaWlpxMfHF9gmt7Fjx5KQkGB7HD9+vCSHI0WVdg7WjYb5PaxBx7823L8cev1HQUdERK6YQ8OOMYZhw4axbNkyvvzyS8LDw+2Wh4eHExoaysaNG23z0tLS2LJlCxEREQC0bt0ad3d3uzYxMTEcOHDA1iY3T09P/P397R7iILZjc96yTl8/6MKZVtptJSIiJcOhu7Eef/xxFi9ezMqVK/Hz87NVcAICAvD29sZisRAdHc2kSZNo2LAhDRs2ZNKkSfj4+NCvXz9b2wcffJCRI0cSFBREYGAgo0aNokWLFnTu3NmRw5NLSTsHn0+8GHL8a0Pv16BBJ8f2S0REnI5Dw87s2bMB6Nixo938efPmMXjwYABGjx5NcnIyQ4cOJT4+nrZt27Jhwwb8/C5eSG7GjBm4ubnRt29fkpOT6dSpE/Pnz8fV1bWshiJF8dvXF47NOWqdvn4QRL2sXVYiIlIqytV1dhxF19kpI2nn4IsX4Zs51mlVc0RE5AoU9u93uTkbS5zcse2wYmiOas7AC9WcAMf2S0REnJ7CjpQuWzXnLcCAf60L1RwdTyUiImVDYUdKj6o5IiJSDijsSPFlZVoDTdIpqBIC9SLAxRXSzuc4NkfVHBERcSyFHSmeQ6tg/RhIPHlxnn8YtH4A9i+6WM1pdT90/T9Vc0RExGEUdqToDq2CjwYCuU7kSzwJm162fu9fC3q9Bg1VzREREcdS2JGiycq0VnRyB52c3H3g0W3gE1hm3RIRESlIubg3llQgx7bb77rKT/p5OHWwbPojIiJyGQo7UjRJp0q2nYiISClT2JGi8a5WuHZV8r/jvIiISFnTMTtSeCf3wfqxl2lksZ6VVS//O86LiIiUNYUdubyMNPjqFdj6KphM8PSH1ETAgv2Byhbrl25TrNfbERERKQe0G0suLfYAvHsbfDXNGnSuuQue2A99F4B/Tfu2/mHQ9wNo1tshXRUREcmPKjuSv8wM+HoGbJ4KWengHQg9XoXmd1mXN+sNTXrkfwVlERGRckRhR/L68ydY/iic/NY63bgH9JoJVYLt27m4QniHMu+eiIhIUSjsyEVZmbDjTfjyZchMtd7iofsrcG1fsFgc3TsREZFiUdgRq9O/WO9QfnyndbpBF+vNO/3DHNsvERGRK6SwU9llZcHud2DjeMhIBg8/6DbJegNPVXNERMQJKOxUZvHHYOXj8NtW63T4LdDnTaha17H9EhERKUEKO5WRMbB3Pmx4DtKSrDfu7PIitHkQXHQ1AhERcS4KO5VNwh+wajj88oV1um57azUn6GrH9ktERKSUKOxUFsbAd0vg02cgNQHcvKDTC9D2UV0bR0REnJrCTmVw9hSsfhIOf2qdrtUa7pgDNRo5tl8iIiJlQGHHmRkDBz6BdaMgOR5c3OHWsRDxJLhq04uISOWgv3jO6txfsHYEHFppnQ69Fu6cAyHXOLZfIiIiZUxhxxn9uBrWPAXn/gQXN+gwCm4ZBa7uju6ZiIhImVPYcSbJ8bBuNPzwkXU6uBncMRvCWjq0WyIiIo6ksOMsDm+wnlKeFAsWF7jpSeg4Ftw8Hd0zERERh1LYqehSEuCzZ2HfQut0UENrNafODY7tl4iISDmhsFOR/bIJVg6DxBOABdoNhU7Pg7u3o3smIiJSbijsVESpSbDxBdgz1zpdrb61mlMvwqHdEhERKY8Udiqa376GlUMh/jfr9A0PQeeJ4FnFod0SEREprxR2Kor0ZPjiRdg5GzAQUAf6vAFXdXR0z0RERMo1hZ2K4PhuWPEonP7ZOt3qfug6Cbz8HdsvERGRCsDFkS/+1Vdf0atXL8LCwrBYLKxYscJuuTGGCRMmEBYWhre3Nx07duTgwYN2bVJTUxk+fDjVq1fH19eX3r17c+LEiTIcRSnKSIWN4+G9KGvQ8asJ/T62VnQUdERERArFoWHn3LlzXHfddbzxxhv5Lp82bRrTp0/njTfeYPfu3YSGhtKlSxfOnj1raxMdHc3y5ctZunQp27ZtIykpiZ49e5KZmVlWwygdJ/fBW5Hw9UwwWXDtPTB0BzSKcnTPREREKhSLMcY4uhMAFouF5cuXc8cddwDWqk5YWBjR0dGMGTMGsFZxQkJCmDp1Ko888ggJCQnUqFGDBQsWcM899wBw8uRJ6tSpw7p16+jatWuhXjsxMZGAgAASEhLw93dwxSQjDbb+G776N5hM8K0BPWdC056O7ZeIiEg5U9i/3w6t7FzK0aNHiY2NJSrqYiXD09OTyMhItm/fDsDevXtJT0+3axMWFkbz5s1tbfKTmppKYmKi3aNcOHUQ3r0Ntky1Bp1md8DQnQo6IiIiV6Dchp3Y2FgAQkJC7OaHhITYlsXGxuLh4UG1atUKbJOfyZMnExAQYHvUqVOnhHtfRJkZ1krOW5EQ+wN4V4N/vgd93wff6o7tm4iISAVXbsNONovFYjdtjMkzL7fLtRk7diwJCQm2x/Hjx0ukr8Xy50/WA5C/fAmy0qHx7TD0G2j+D8f1SURExImU21PPQ0NDAWv1pmbNmrb5cXFxtmpPaGgoaWlpxMfH21V34uLiiIgo+GrCnp6eeHo6+AaZWZmwcxZ88RJkpoJnAHSfCtfdC5cJcyIiIlJ45bayEx4eTmhoKBs3brTNS0tLY8uWLbYg07p1a9zd3e3axMTEcODAgUuGHYc7/QvM7wEbnrMGnas7Wc+0anmfgo6IiEgJc2hlJykpiZ9//tk2ffToUfbv309gYCB169YlOjqaSZMm0bBhQxo2bMikSZPw8fGhX79+AAQEBPDggw8ycuRIgoKCCAwMZNSoUbRo0YLOnTs7alhWWZlwbDsknYIqIRfuW2WB3e/C5+Mh/Tx4VIGu/wfXD1LIERERKSUODTt79uzh1ltvtU2PGDECgEGDBjF//nxGjx5NcnIyQ4cOJT4+nrZt27Jhwwb8/Pxsz5kxYwZubm707duX5ORkOnXqxPz583F1dS3z8dgcWgXrx0DiyYvzqoSATxDEHbJO1+8Afd6EavUc00cREZFKotxcZ8eRSvQ6O4dWwUcDgQLeVlcPiPo/6w08XcrtXkQREZFyr8JfZ6dCysq0VnQKCjpgPa38hgcVdERERMqI/uKWpGPb7Xdd5SfplLWdiIiIlAmFnZKUdKpk24mIiMgVU9gpSVVCLt+mKO1ERETkiinslKR6EeAfBhR0GrkF/GtdOA1dREREyoLCTklycYVuUy9M5A48F6a7TbG2ExERkTKhsFPSmvWGvh+Af037+f5h1vnNejumXyIiIpVUub03VoXWrDc06ZH3Csqq6IiIiJQ5hZ3S4uIK4R0c3QsREZFKT7uxRERExKkp7IiIiIhTU9gRERERp6awIyIiIk5NByiLiIhIqcjMMuw6+jdxZ1MI9vPixvBAXF0KuvBu6VHYERERkRK3/kAME1cfIiYhxTavZoAX43s1o1vzmpd4ZsnTbiwREREpUesPxPDYwm/tgg5AbEIKjy38lvUHYsq0Pwo7IiIiUmIyMrN4YeVBTD7LsudNXH2IzKz8WpQO7cYSERGRQjHGcOZ8OicTkolNSOFkQgqxCcnEnEmxzfsjPpn0SwQZA8QkpLDr6N+0vzqoTPqtsCMiIiIYY0hITufkmRRiE5OtXxMuhpiYhBRiEpJJSc8qkdeLO5ty+UYlRGFHRETEQcrqbCVjDInJGTkqMhe+Xgg2MWesYSY5PbNQ66texYPQAC9qBnhT0+6rF6cSU3hi6f7LriPYz+sKR1V4CjsiIiIOUFJnKxljSEzJsIWYmDPWXUvWXUwXg835tMIFmUBfD/sAU9XLbjrE3wsv94JvbJ2ZZZj86f+ITUjJ97gdCxAaYA12ZUVhR0REpIxln62UOwxkn600e8D1tsCTmJJ+oQpz8TiZmDPJxCZenHeukEGmmo97rhBjX5kJDbh0kCkMVxcL43s147GF32IBuzFm16zG92pWptfbsRhjyu5w6HIqMTGRgIAAEhIS8Pf3d3R3RETEiWVmGW6a8iWxiQUfs+Lp5kLtat6cSkwlKTWjUOutmjPIBHjlqs5Yv15pkCmKsrjOTmH/fquyIyIiUgSZWYak1AzOpqSTmGz9ejYlg7OpF76mZJCYe1nKxWVnzqeRknHpg3xTM7L45c9ztukAb/eLAaaqNzX9LwaY7IqMj0f5+pPerXlNujQL1RWURUREylJWliEpLcMWQHIHksSUDLtwkphPWClspeVKDe14Nf9oXZtQfy98PSvmn2tXF0uZnV5+KRXz3RMRkXJz36GykpVlOGcLKvaBJDEl/ypKYrL9vKS0DErq4A0PNxf8vdzw83LHz8sNPy83/G3f23/1ty1z5+e4szz10XeXXX+HhjW4ukaVkulsJaewIyJSAZWn+w4VhjGGc2mZ+VZRcgcSu7CSI9AkpZZgUHF1sQWUvGGl4PDi731xmadb8Y5/aRbmz7TPfipXZys5O4UdEZEKpihn8pQEYwzn0zJzhZR0uwpLYp7pjDwBpqTuDuDuarELJH6eeQOJf54gYx9gyvJA3dzK49lKzk5hR0SkAsnMMkxcfajA+w5ZsN53qEuzUFxdLBhjSEnPuhBG8g8hOasrdkEmxwG3JXUfI1cXS55dPxd39bjnW03J/j77eV7uLlgsFTsIdGtek9kDrs9TnQstx9W5ikynnqNTz0WkfDHGkJqRxbnUDM6lZnIuLYPzadbv9/0ez4zPj1x2HcF+nqRnZnE2JYOMEgoqLhbyBJOLIcS+iuLvbX+8Svb33u6uFT6olKTKdtxVSdOp5yIiZSQ1I5PzF0KJLZzYpjM4l5bJ+dQc36dlkJR6YV5aBufTMklKvfic82mZV1xJiTubajftYoEqnvZVlJyBJE9YyfXVz8sNHw8FlZJWXs5WcnYKO1Js+o+k4quM2zA9MytHqMgZOrJDSP5BJSnVujy/4JKeWXoFcm93V3w9XfHxcMPX043MrCwOn0q67PMm9GpGRIPqtvDiq6AilZjCjhRLRTsTRPKqCNswM8vYdt9kV0uSUjNsoePchdBxPu1iOMmvwpLzOWmXuZjblfB0c8HX01oB8fVww9fT9eK0pxu+Hm74eLpSxcMNH083fD1c8fF0o0p2mMn1HB8PtzzhMzPLcPPULy97Js/97es7fXAVKSwds0PpHLPjzP8xF3QmSPboSvpMECl5pbENs7IMyek5gkZ2CMneVZMrdOSultgFlwvPSUkvvWDi7mq5GEDyhI4L4eRC6Kji6XahsuJq+2ofTKzt3F1dSq2/OWVvP8j/TB79DEplUemO2Zk1axavvPIKMTExXHPNNcycOZMOHTo4pC8V4T/mosg+WDL5wnEFz684UOCZIADPLj+At7srLi4WLFwMeDkr6Haxz27+5dvnLsUX3C7/Fyn0egt8jfzXVXCfCm5fmHZXMibrMovdsswsw/MrDl5yGz7zyQ/EXLhLcnZl5XIVlsLeUbk4XCzg6+l2IXS45htCfHNUS6xBJf9qSfZzPNzKJpiUBp3JI1I0TlHZ+fDDD7n//vuZNWsWN910E2+99Rbvvvsuhw4dom7dupd9fklWdsqy6pGVZUjJyCQlPYvk9EyS0zJJSc8kOf3C17SL3xfYJj0rn3mZpNiea32eSGFZLNiqJVU8rbtt7Kol2btycgUVu6pJjoqLr6cbnm4V/1Tj0uDMFWSRwijs32+nCDtt27bl+uuvZ/bs2bZ5TZs25Y477mDy5MmXfX5JhZ3sfek5/9PKLdDXg5f7NCc9K8sWRrLDRUpGfvOyQ0vWhdCSaQstqaV47EFBXC1QmGMxwwK88Pd2t03n/JSZHFHQfn6O73MsyPNyRXxOoV77EmO6ovUW0D730qK+D/Z9t1+xKWAi+9v0zKxCfXZa1gmgYbBfrmNOXC9UT/Kvlvh6uurUYhEpM5VmN1ZaWhp79+7lmWeesZsfFRXF9u3b831OamoqqakXT8tMTEwskb7sOvr3JYMOwN/n0hi6+NsSeb2cPNxc8HZ3xcs9+6sr3h6uF7+3zXPBy826LHu+9XuXfNpe/N7L3QUvd1f2/BbPfe/svGx/Xu3bUqdTllM7fjldqG04pltTbUMRcQoVPuz89ddfZGZmEhISYjc/JCSE2NjYfJ8zefJkJk6cWOJ9iTt76aCTLby6L2FVvfINF7YAciFc5Jx3MZy42M3zcncts9L1jeGB1Azw0j1dKjBtQxGpbCp82MmWu2xujCmwlD527FhGjBhhm05MTKROnTpX3IdgP69CtZt0Z4sK+x+z7ulS8WkbikhlU3FPR7igevXquLq65qnixMXF5an2ZPP09MTf39/uURKy/2Mu6E+EBetZWRX9P+bsM0FCA+zDXWiAl055rSC0DUWkMqnwlR0PDw9at27Nxo0bufPOO23zN27cSJ8+fcq0L5XpP+ZuzWvSpVmozgSpwLQNRaSyqPBhB2DEiBHcf//9tGnThvbt2/P222/z+++/8+ijj5Z5XyrT9S90T5eKT9tQRCoDpwg799xzD6dPn+bFF18kJiaG5s2bs27dOurVq+eQ/ug/ZhERkfLDKa6zc6VK43YRIiIiUroK+/e7wh+gLCIiInIpCjsiIiLi1BR2RERExKkp7IiIiIhTU9gRERERp6awIyIiIk5NYUdEREScmsKOiIiIODWFHREREXFqTnG7iCuVfRHpxMREB/dERERECiv77/blbgahsAOcPXsWgDp16ji4JyIiIlJUZ8+eJSAgoMDlujcWkJWVxcmTJ/Hz88NiKbmbdSYmJlKnTh2OHz/utPfccvYxOvv4wPnHqPFVfM4+Ro2v+IwxnD17lrCwMFxcCj4yR5UdwMXFhdq1a5fa+v39/Z3yA5yTs4/R2ccHzj9Gja/ic/YxanzFc6mKTjYdoCwiIiJOTWFHREREnJrCTiny9PRk/PjxeHp6OrorpcbZx+js4wPnH6PGV/E5+xg1vtKnA5RFRETEqamyIyIiIk5NYUdEREScmsKOiIiIODWFHREREXFqCjtFNGvWLMLDw/Hy8qJ169Zs3br1ku23bNlC69at8fLy4qqrrmLOnDl2y+fPn4/FYsnzSElJKc1hFKgo44uJiaFfv340btwYFxcXoqOj8233ySef0KxZMzw9PWnWrBnLly8vpd4XTkmPsSJvw2XLltGlSxdq1KiBv78/7du357PPPsvTrjxtw5IeX3nbflC0MW7bto2bbrqJoKAgvL29adKkCTNmzMjTrqJuw8KMr7xtw6L+ncj29ddf4+bmRsuWLfMsK0/bD0p+jKW+DY0U2tKlS427u7t55513zKFDh8yTTz5pfH19zbFjx/Jt/+uvvxofHx/z5JNPmkOHDpl33nnHuLu7m//+97+2NvPmzTP+/v4mJibG7uEIRR3f0aNHzRNPPGHef/9907JlS/Pkk0/mabN9+3bj6upqJk2aZH788UczadIk4+bmZnbu3FnKo8lfaYyxIm/DJ5980kydOtXs2rXLHD582IwdO9a4u7ubb7/91tamPG3D0hhfedp+xhR9jN9++61ZvHixOXDggDl69KhZsGCB8fHxMW+99ZatTUXehoUZX3nahkUdX7YzZ86Yq666ykRFRZnrrrvObll52n7GlM4YS3sbKuwUwY033mgeffRRu3lNmjQxzzzzTL7tR48ebZo0aWI375FHHjHt2rWzTc+bN88EBASUeF+Lo6jjyykyMjLfINC3b1/TrVs3u3ldu3Y199577xX1tbhKY4zOsg2zNWvWzEycONE2XZ62YWmMrzxtP2NKZox33nmnGTBggG3a2bZh7vGVp21Y3PHdc8895rnnnjPjx4/PEwTK0/YzpnTGWNrbULuxCiktLY29e/cSFRVlNz8qKort27fn+5wdO3bkad+1a1f27NlDenq6bV5SUhL16tWjdu3a9OzZk3379pX8AC6jOOMrjILegytZZ3GV1hjBebZhVlYWZ8+eJTAw0DavvGzD0hoflI/tByUzxn379rF9+3YiIyNt85xpG+Y3Pigf27C445s3bx6//PIL48ePz3d5edl+UHpjhNLdhgo7hfTXX3+RmZlJSEiI3fyQkBBiY2PzfU5sbGy+7TMyMvjrr78AaNKkCfPnz2fVqlUsWbIELy8vbrrpJo4cOVI6AylAccZXGAW9B1eyzuIqrTE60zZ89dVXOXfuHH379rXNKy/bsLTGV162H1zZGGvXro2npydt2rTh8ccf56GHHrItc4ZteKnxlZdtWJzxHTlyhGeeeYZFixbh5pb/vbnLy/aD0htjaW9D3fW8iCwWi920MSbPvMu1zzm/Xbt2tGvXzrb8pptu4vrrr+f111/ntddeK6luF1pRx+eodV6Jku6Ps2zDJUuWMGHCBFauXElwcHCJrLM0lPT4ytv2g+KNcevWrSQlJbFz506eeeYZGjRowH333XdF6ywtJT2+8rYNCzu+zMxM+vXrx8SJE2nUqFGJrLOslPQYS3sbKuwUUvXq1XF1dc2TXOPi4vIk3GyhoaH5tndzcyMoKCjf57i4uHDDDTeU+X8kxRlfYRT0HlzJOourtMaYW0Xchh9++CEPPvggH3/8MZ07d7ZbVl62YWmNLzdHbT+4sjGGh4cD0KJFC06dOsWECRNsYcAZtuGlxpdbRfkZPHv2LHv27GHfvn0MGzYMsO5qNcbg5ubGhg0buO2228rN9oPSG2NuJb0NtRurkDw8PGjdujUbN260m79x40YiIiLyfU779u3ztN+wYQNt2rTB3d093+cYY9i/fz81a9YsmY4XUnHGVxgFvQdXss7iKq0x5lbRtuGSJUsYPHgwixcvpkePHnmWl5dtWFrjy81R2w9K7jNqjCE1NdU2XdG3YW65x5ff8orwM+jv788PP/zA/v37bY9HH32Uxo0bs3//ftq2bQuUn+0HpTfG3Ep8G5baoc9OKPt0u7lz55pDhw6Z6Oho4+vra3777TdjjDHPPPOMuf/++23ts089f+qpp8yhQ4fM3Llz85x6PmHCBLN+/Xrzyy+/mH379pkhQ4YYNzc3880335T78RljzL59+8y+fftM69atTb9+/cy+ffvMwYMHbcu//vpr4+rqaqZMmWJ+/PFHM2XKlHJxymRJjrEib8PFixcbNzc38+abb9qd7nnmzBlbm/K0DUtjfOVp+xlT9DG+8cYbZtWqVebw4cPm8OHD5r333jP+/v5m3LhxtjYVeRsWZnzlaRsW53dMTvmdqVSetp8xpTPG0t6GCjtF9Oabb5p69eoZDw8Pc/3115stW7bYlg0aNMhERkbatd+8ebNp1aqV8fDwMPXr1zezZ8+2Wx4dHW3q1q1rPDw8TI0aNUxUVJTZvn17WQwlX0UdH5DnUa9ePbs2H3/8sWncuLFxd3c3TZo0MZ988kkZjKRgJT3GirwNIyMj8x3foEGD7NZZnrZhSY+vvG0/Y4o2xtdee81cc801xsfHx/j7+5tWrVqZWbNmmczMTLt1VtRtWJjxlbdtWNTfMTnlFwSMKV/bz5iSH2Npb0OLMReOmBURERFxQjpmR0RERJyawo6IiIg4NYUdERERcWoKOyIiIuLUFHZERETEqSnsiIiIiFNT2BERERGnprAjIiIiTk1hR0Sc1vbt27FYLHTr1s3RXRERB9IVlEXEaT300EOcP3+eTz75hCNHjlC3bl1Hd0lEHECVHRFxSufOnePDDz8kOjqa2267jfnz5zu6SyLiIAo7IuKUPvzwQ0JDQ7nxxhvp378/8+bNQ4VskcpJYUdEnNLcuXPp378/AHfccQdxcXF88cUXDu6ViDiCjtkREafz008/0aRJE3766ScaNWoEQL9+/QBYvHixI7smIg6gyo6IOJ25c+dyww032IIOQP/+/Vm2bBnx8fEO7JmIOILCjog4lYyMDD744ANbJSdb165d8fPzY9GiRQ7qmYg4ipujOyAiUpLWrFnDqVOnaN68OQcOHLBb1qFDB+bOncuwYcMc1DsRcQQdsyMiTqVXr16sWbPmkm327t3L9ddfX0Y9EhFHU9gRERERp6ZjdkRERMSpKeyIiIiIU1PYEREREaemsCMiIiJOTWFHREREnJrCjoiIiDg1hR0RERFxago7IiIi4tQUdkRERMSpKeyIiIiIU1PYEREREaemsCMiIiJO7f8B/tHY4UuIlh0AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# write your code for the above part here\n",
        "# write your code for the above part here\n",
        "# write your code for the above part here\n",
        "# Import necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "from gymnasium.utils import seeding\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class ArmedBanditsEnv(gym.Env):\n",
        "    def __init__(self, delta):\n",
        "        super(ArmedBanditsEnv, self).__init__()\n",
        "        #define action and observation space\n",
        "        self.delta = delta\n",
        "        self.action_space = spaces.Discrete(2) #two arms\n",
        "        self.observation_space = spaces.Discrete(1)\n",
        "        self.true_means = [0.5, 0.5 + self.delta]\n",
        "        self.step_count = 0 #time step count\n",
        "        self.seed_value = None\n",
        "    def reset(self):\n",
        "        self.step_count = 0\n",
        "        return np.array([0.0])\n",
        "    def step(self, action):\n",
        "        reward = np.random.binomial(1, self.true_means[action])\n",
        "        self.step_count += 1\n",
        "        done = self.step_count >= T\n",
        "        return np.array([0.0]), reward, done, {}\n",
        "    def render(self):\n",
        "         pass\n",
        "    def seed(self, seed = None):\n",
        "        self.seed_value = seed\n",
        "        np.random.seed(seed) \n",
        "    def close(self):\n",
        "        pass #cleanup and closing logic\n",
        "\n",
        "#ETC Algorithm  \n",
        "def improved_etc_algorithm(env, T, delta):\n",
        "        m = int(np.ceil(np.log(2*T)/delta**2)) #based on delta value (sub optimality)\n",
        "        total_reward = 0\n",
        "\n",
        "        for n in range(T):\n",
        "            if n % m == 0:\n",
        "                action = np.random.randint(2)\n",
        "            else:\n",
        "                action = np.argmax([0.5, 0.5 + env.delta])\n",
        "\n",
        "            _, reward, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            regret = T *(0.5 + env.delta) - total_reward\n",
        "        return regret\n",
        "#parameters   \n",
        "#monte carlo simulations\n",
        "number_of_experiments = 500\n",
        "delta_values = [0.05,0.1,0.2,0.3,0.4,0.45]\n",
        "c_ucb = 2.0\n",
        "T = 10000\n",
        "exploration_factor = 1.0\n",
        "regret_results_improved_etc= []\n",
        "regret_results_ucb = []\n",
        "\n",
        "for delta in delta_values:\n",
        "    total_regret_improved_etc = 0\n",
        "    total_regret_ucb = 0\n",
        "    \n",
        "    \n",
        "    for _ in range(number_of_experiments):\n",
        "        env_improved_etc = ArmedBanditsEnv(delta)\n",
        "        regret_improved_etc = improved_etc_algorithm(env_improved_etc, T, delta)\n",
        "        total_regret_improved_etc += regret_improved_etc\n",
        "        \n",
        "        env_ucb = ArmedBanditsEnv(delta)\n",
        "        regret_ucb = improved_etc_algorithm(env_ucb, T, c_ucb)\n",
        "        total_regret_ucb += regret_ucb\n",
        "\n",
        "    avg_regret_improved_etc = total_regret_improved_etc / number_of_experiments\n",
        "    regret_results_improved_etc.append(avg_regret_improved_etc)\n",
        "\n",
        "    avg_regret_ucb = total_regret_ucb / number_of_experiments\n",
        "    regret_results_ucb.append(avg_regret_ucb)\n",
        "\n",
        "plt.plot(delta_values, regret_results_improved_etc, marker= 'o', label = 'Improved ETC')\n",
        "plt.plot(delta_values, regret_results_ucb, marker= 'o',label ='UCB')\n",
        "plt.xlabel('$\\Delta$')\n",
        "plt.ylabel('Regret')\n",
        "plt.title('Comparision of Improved ETC and UCB Algorithms')\n",
        "plt.legend()\n",
        "#show the figure\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
